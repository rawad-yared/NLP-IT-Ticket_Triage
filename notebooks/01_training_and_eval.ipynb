{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f8ee623",
   "metadata": {},
   "source": [
    "# IT Ticket Triage - Training & Evaluation (Dept + Urgency + Tags/Summary)\n",
    "\n",
    "This notebook trains and evaluates the NLP triage pipeline in phases.\n",
    "\n",
    "- Trains: department routing classifier and urgency/priority classifier\n",
    "- Prototypes: tag extraction and summary generation\n",
    "- Produces: metrics, saved model artifacts, and inference-ready mappings\n",
    "\n",
    "Target inference JSON schema:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"ticket_id\": \"...\",\n",
    "  \"department\": {\"label\": \"...\", \"confidence\": 0.92},\n",
    "  \"urgency\": {\"label\": \"...\", \"confidence\": 0.81},\n",
    "  \"tags\": [\"vpn\", \"login\", \"timeout\"],\n",
    "  \"summary\": \"User cannot connect to VPN after password reset.\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f8c83fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.4\n",
      "Torch: 2.10.0\n",
      "CUDA available: False\n",
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Environment sanity + GPU check + deterministic seeds\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "print(f\"Python: {sys.version.split()[0]}\")\n",
    "print(f\"Torch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(SEED)\n",
    "print(f\"Seed set to {SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a0ffb5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local environment detected. Skipping pip install in notebook; using active .venv packages.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Install dependencies (single requirements cell)\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    subprocess.check_call([\n",
    "        sys.executable,\n",
    "        \"-m\",\n",
    "        \"pip\",\n",
    "        \"install\",\n",
    "        \"-q\",\n",
    "        \"-U\",\n",
    "        \"numpy==1.26.4\",\n",
    "        \"pandas==2.2.2\",\n",
    "        \"scikit-learn>=1.4,<2\",\n",
    "        \"transformers<5\",\n",
    "        \"datasets<3\",\n",
    "        \"evaluate<1\",\n",
    "        \"accelerate<1\",\n",
    "        \"yake<1\",\n",
    "        \"sentencepiece<1\",\n",
    "    ])\n",
    "    print(\"Colab dependencies installed.\")\n",
    "else:\n",
    "    print(\"Local environment detected. Skipping pip install in notebook; using active .venv packages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a84ca0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rawadyared/NLP-IT-Ticket_Triage/.venv/bin/python\n",
      "numpy: 2.1.3\n",
      "pandas: 2.2.3\n"
     ]
    }
   ],
   "source": [
    "import sys, numpy, pandas\n",
    "print(sys.executable)\n",
    "print(\"numpy:\", numpy.__version__)\n",
    "print(\"pandas:\", pandas.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e57db188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Imports\n",
    "import json\n",
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import datasets\n",
    "import evaluate\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, EarlyStoppingCallback, Trainer, TrainingArguments\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_colwidth\", 160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "501868b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolved BASE_DIR: /Users/rawadyared/NLP-IT-Ticket_Triage\n",
      "{\n",
      "  \"dataset_path\": \"/Users/rawadyared/NLP-IT-Ticket_Triage/data/processed/IT Support Ticket Data.stratified_3000.csv\",\n",
      "  \"id_column\": \"Unnamed: 0\",\n",
      "  \"text_columns\": [\n",
      "    \"Body\"\n",
      "  ],\n",
      "  \"label_columns\": {\n",
      "    \"department\": \"Department\",\n",
      "    \"urgency\": \"Priority\"\n",
      "  },\n",
      "  \"model_names\": {\n",
      "    \"department\": \"distilroberta-base\",\n",
      "    \"urgency\": \"distilroberta-base\",\n",
      "    \"summary\": \"t5-small\"\n",
      "  },\n",
      "  \"candidate_models\": {\n",
      "    \"department\": [\n",
      "      \"distilroberta-base\"\n",
      "    ],\n",
      "    \"urgency\": [\n",
      "      \"distilroberta-base\"\n",
      "    ]\n",
      "  },\n",
      "  \"train\": {\n",
      "    \"max_length\": 256,\n",
      "    \"batch_size\": 8,\n",
      "    \"learning_rate\": 2e-05,\n",
      "    \"learning_rates\": [\n",
      "      1e-05,\n",
      "      2e-05,\n",
      "      3e-05\n",
      "    ],\n",
      "    \"epochs\": 3,\n",
      "    \"weight_decay\": 0.01,\n",
      "    \"warmup_ratio\": 0.1,\n",
      "    \"early_stopping_patience\": 1\n",
      "  },\n",
      "  \"split\": {\n",
      "    \"train_size\": 0.8,\n",
      "    \"val_size\": 0.1,\n",
      "    \"test_size\": 0.1\n",
      "  },\n",
      "  \"preprocess\": {\n",
      "    \"lowercase\": false,\n",
      "    \"remove_boilerplate\": true,\n",
      "    \"remove_urls\": true,\n",
      "    \"remove_emails\": true\n",
      "  },\n",
      "  \"label_standardization\": {\n",
      "    \"department_aliases\": {\n",
      "      \"tech support\": \"Technical Support\",\n",
      "      \"technical support\": \"Technical Support\",\n",
      "      \"it support\": \"IT Support\",\n",
      "      \"billing & payments\": \"Billing and Payments\"\n",
      "    },\n",
      "    \"urgency_aliases\": {\n",
      "      \"urgent\": \"high\",\n",
      "      \"high priority\": \"high\",\n",
      "      \"med\": \"medium\",\n",
      "      \"normal\": \"medium\",\n",
      "      \"low priority\": \"low\"\n",
      "    }\n",
      "  },\n",
      "  \"paths\": {\n",
      "    \"results_dir\": \"/Users/rawadyared/NLP-IT-Ticket_Triage/results\",\n",
      "    \"models_dir\": \"/Users/rawadyared/NLP-IT-Ticket_Triage/models\",\n",
      "    \"mappings_dir\": \"/Users/rawadyared/NLP-IT-Ticket_Triage/results/mappings\"\n",
      "  },\n",
      "  \"experiment\": {\n",
      "    \"candidate_epochs\": 3,\n",
      "    \"run_candidate_search\": true,\n",
      "    \"tune_learning_rate\": true\n",
      "  },\n",
      "  \"seed\": 42\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Repo paths + config (single source of truth)\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "if \"SEED\" not in globals():\n",
    "    SEED = 42\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "def detect_base_dir() -> Path:\n",
    "    if IN_COLAB:\n",
    "        preferred = Path(\"/content/NLP-IT-Ticket_Triage\")\n",
    "        if preferred.exists():\n",
    "            return preferred\n",
    "        return Path(\"/content\")\n",
    "\n",
    "    cwd = Path.cwd().resolve()\n",
    "    candidate_processed = Path(\"data\") / \"processed\" / \"IT Support Ticket Data.stratified_3000.csv\"\n",
    "    candidate_raw = Path(\"data\") / \"raw\" / \"IT Support Ticket Data.csv\"\n",
    "\n",
    "    search_roots = [cwd, *cwd.parents]\n",
    "    for root in search_roots:\n",
    "        if (root / candidate_processed).exists() or (root / candidate_raw).exists():\n",
    "            return root\n",
    "    return cwd\n",
    "\n",
    "BASE_DIR = detect_base_dir()\n",
    "print(f\"Resolved BASE_DIR: {BASE_DIR}\")\n",
    "\n",
    "DATASET_CANDIDATES = [\n",
    "    BASE_DIR / \"data\" / \"processed\" / \"IT Support Ticket Data.stratified_3000.csv\",\n",
    "    BASE_DIR / \"data\" / \"processed\" / \"IT Support Ticket Data.stratified_1000.csv\",\n",
    "    BASE_DIR / \"data\" / \"raw\" / \"IT Support Ticket Data.csv\",\n",
    "    Path(\"/content\") / \"IT Support Ticket Data.stratified_3000.csv\",\n",
    "    Path(\"/content\") / \"IT Support Ticket Data.stratified_1000.csv\",\n",
    "    Path(\"/content\") / \"IT Support Ticket Data.csv\",\n",
    "]\n",
    "\n",
    "\n",
    "def resolve_dataset_path(candidates):\n",
    "    for path in candidates:\n",
    "        if path.exists():\n",
    "            return path\n",
    "    candidate_list = \"\\n\".join([str(p) for p in candidates])\n",
    "    raise FileNotFoundError(f\"Dataset not found. Checked:\\n{candidate_list}\")\n",
    "\n",
    "OUTPUT_ROOT = Path(\"/content\") if IN_COLAB else BASE_DIR\n",
    "RESULTS_DIR = OUTPUT_ROOT / \"results\"\n",
    "MODELS_DIR = OUTPUT_ROOT / \"models\"\n",
    "MAPPINGS_DIR = RESULTS_DIR / \"mappings\"\n",
    "\n",
    "for out_dir in [RESULTS_DIR, MODELS_DIR, MAPPINGS_DIR]:\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CONFIG = {\n",
    "    \"dataset_path\": str(resolve_dataset_path(DATASET_CANDIDATES)),\n",
    "    \"id_column\": \"Unnamed: 0\",\n",
    "    \"text_columns\": [\"Body\"],\n",
    "    \"label_columns\": {\n",
    "        \"department\": \"Department\",\n",
    "        \"urgency\": \"Priority\",\n",
    "    },\n",
    "    \"model_names\": {\n",
    "        \"department\": \"distilroberta-base\",\n",
    "        \"urgency\": \"distilroberta-base\",\n",
    "        \"summary\": \"t5-small\",\n",
    "    },\n",
    "    \"candidate_models\": {\n",
    "        \"department\": [\"distilroberta-base\"],\n",
    "        \"urgency\": [\"distilroberta-base\"],\n",
    "    },\n",
    "    \"train\": {\n",
    "        \"max_length\": 256,\n",
    "        \"batch_size\": 8,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"learning_rates\": [1e-5, 2e-5, 3e-5],\n",
    "        \"epochs\": 3,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        \"early_stopping_patience\": 1,\n",
    "    },\n",
    "    \"split\": {\n",
    "        \"train_size\": 0.8,\n",
    "        \"val_size\": 0.1,\n",
    "        \"test_size\": 0.1,\n",
    "    },\n",
    "    \"preprocess\": {\n",
    "        \"lowercase\": False,\n",
    "        \"remove_boilerplate\": True,\n",
    "        \"remove_urls\": True,\n",
    "        \"remove_emails\": True\n",
    "    },\n",
    "    \"label_standardization\": {\n",
    "        \"department_aliases\": {\n",
    "            \"tech support\": \"Technical Support\",\n",
    "            \"technical support\": \"Technical Support\",\n",
    "            \"it support\": \"IT Support\",\n",
    "            \"billing & payments\": \"Billing and Payments\"\n",
    "        },\n",
    "        \"urgency_aliases\": {\n",
    "            \"urgent\": \"high\",\n",
    "            \"high priority\": \"high\",\n",
    "            \"med\": \"medium\",\n",
    "            \"normal\": \"medium\",\n",
    "            \"low priority\": \"low\"\n",
    "        }\n",
    "    },\n",
    "    \"paths\": {\n",
    "        \"results_dir\": str(RESULTS_DIR),\n",
    "        \"models_dir\": str(MODELS_DIR),\n",
    "        \"mappings_dir\": str(MAPPINGS_DIR),\n",
    "    },\n",
    "    \"experiment\": {\n",
    "        \"candidate_epochs\": 3,\n",
    "        \"run_candidate_search\": True,\n",
    "        \"tune_learning_rate\": True\n",
    "    },\n",
    "    \"seed\": SEED,\n",
    "}\n",
    "\n",
    "print(json.dumps(CONFIG, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "46a57edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (3000, 5)\n",
      "Columns: ['Unnamed: 0', 'Body', 'Department', 'Priority', 'Tags']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Body</th>\n",
       "      <th>Department</th>\n",
       "      <th>Priority</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27907</td>\n",
       "      <td>Hello Customer Support, I have contacted you to seek information about the integrations supported by your project management software as a service. I am par...</td>\n",
       "      <td>Technical Support</td>\n",
       "      <td>low</td>\n",
       "      <td>['Product', 'Integration', 'Documentation', 'API', 'Guidance']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2966</td>\n",
       "      <td>Our marketing agency faced several hardware and software failures, which affected our digital campaigns. The possible root cause was compatibility issues. T...</td>\n",
       "      <td>Technical Support</td>\n",
       "      <td>high</td>\n",
       "      <td>['Hardware', 'Software', 'Performance', 'Disruption', 'Outage']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16253</td>\n",
       "      <td>In need of comprehensive documentation for integrating ESET NOD32 Antivirus 14 with the SaaS project management platform. Requesting detailed steps for the ...</td>\n",
       "      <td>Technical Support</td>\n",
       "      <td>high</td>\n",
       "      <td>['Documentation', 'Feature', 'Security', 'IT', 'Tech Support']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  \\\n",
       "0       27907   \n",
       "1        2966   \n",
       "2       16253   \n",
       "\n",
       "                                                                                                                                                              Body  \\\n",
       "0  Hello Customer Support, I have contacted you to seek information about the integrations supported by your project management software as a service. I am par...   \n",
       "1  Our marketing agency faced several hardware and software failures, which affected our digital campaigns. The possible root cause was compatibility issues. T...   \n",
       "2  In need of comprehensive documentation for integrating ESET NOD32 Antivirus 14 with the SaaS project management platform. Requesting detailed steps for the ...   \n",
       "\n",
       "          Department Priority  \\\n",
       "0  Technical Support      low   \n",
       "1  Technical Support     high   \n",
       "2  Technical Support     high   \n",
       "\n",
       "                                                              Tags  \n",
       "0   ['Product', 'Integration', 'Documentation', 'API', 'Guidance']  \n",
       "1  ['Hardware', 'Software', 'Performance', 'Disruption', 'Outage']  \n",
       "2   ['Documentation', 'Feature', 'Security', 'IT', 'Tech Support']  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 6: Load dataset\n",
    "df_raw = pd.read_csv(CONFIG[\"dataset_path\"])\n",
    "print(f\"Dataset shape: {df_raw.shape}\")\n",
    "print(f\"Columns: {list(df_raw.columns)}\")\n",
    "display(df_raw.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f16ff539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using text columns: ['Body']\n",
      "Rows after text cleaning: 3000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ticket_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27907</td>\n",
       "      <td>Hello Customer Support, I have contacted you to seek information about the integrations supported by your project management software as a service. I am par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2966</td>\n",
       "      <td>Our marketing agency faced several hardware and software failures, which affected our digital campaigns. The possible root cause was compatibility issues. T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16253</td>\n",
       "      <td>In need of comprehensive documentation for integrating ESET NOD32 Antivirus 14 with the SaaS project management platform. Requesting detailed steps for the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  \\\n",
       "0       27907   \n",
       "1        2966   \n",
       "2       16253   \n",
       "\n",
       "                                                                                                                                                       ticket_text  \n",
       "0  Hello Customer Support, I have contacted you to seek information about the integrations supported by your project management software as a service. I am par...  \n",
       "1  Our marketing agency faced several hardware and software failures, which affected our digital campaigns. The possible root cause was compatibility issues. T...  \n",
       "2  In need of comprehensive documentation for integrating ESET NOD32 Antivirus 14 with the SaaS project management platform. Requesting detailed steps for the ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 7: Build ticket_text\n",
    "configured_cols = CONFIG[\"text_columns\"]\n",
    "available_text_cols = [col for col in configured_cols if col in df_raw.columns]\n",
    "\n",
    "if not available_text_cols:\n",
    "    fallback_cols = [col for col in [\"short_description\", \"description\", \"Body\"] if col in df_raw.columns]\n",
    "    if not fallback_cols:\n",
    "        raise ValueError(\"No usable text columns found. Update CONFIG['text_columns'].\")\n",
    "    available_text_cols = fallback_cols\n",
    "\n",
    "print(f\"Using text columns: {available_text_cols}\")\n",
    "\n",
    "HEADER_LINE_RE = re.compile(r\"(?i)^(from|sent|to|subject|cc|bcc):\")\n",
    "SEPARATOR_LINE_RE = re.compile(r\"^[-_]{2,}$\")\n",
    "GREETING_LINE_RE = re.compile(r\"(?i)^(dear|hi|hello)\\b\")\n",
    "SIGNOFF_MARKER_RE = re.compile(r\"(?i)\\b(best regards|kind regards|regards|thanks(?: and regards)?|thank you|sincerely)\\b\")\n",
    "DISCLAIMER_MARKER_RE = re.compile(r\"(?i)\\b(this email and any attachments are confidential|do not reply to this email)\\b\")\n",
    "\n",
    "def _is_boilerplate_line(line: str) -> bool:\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        return False\n",
    "    if HEADER_LINE_RE.match(line):\n",
    "        return True\n",
    "    if SEPARATOR_LINE_RE.match(line):\n",
    "        return True\n",
    "    if GREETING_LINE_RE.match(line):\n",
    "        # Keep natural ticket text that starts with greeting + issue details.\n",
    "        words = line.rstrip(',:').split()\n",
    "        if len(words) <= 6 and len(line) <= 45:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def _trim_trailing_boilerplate(text: str, marker_re: re.Pattern, min_ratio: float = 0.60) -> str:\n",
    "    matches = list(marker_re.finditer(text))\n",
    "    if not matches:\n",
    "        return text\n",
    "    cutoff = matches[-1].start()\n",
    "    if cutoff >= int(len(text) * min_ratio):\n",
    "        return text[:cutoff]\n",
    "    return text\n",
    "\n",
    "def normalize_ticket_text(text: str) -> str:\n",
    "    text = \"\" if pd.isna(text) else str(text)\n",
    "\n",
    "    # Normalize obvious formatting noise first.\n",
    "    text = re.sub(r\"<[^>]+>\", \" \", text)\n",
    "\n",
    "    if CONFIG.get(\"preprocess\", {}).get(\"remove_urls\", True):\n",
    "        text = re.sub(r\"https?://\\S+|www\\.\\S+\", \" \", text)\n",
    "    if CONFIG.get(\"preprocess\", {}).get(\"remove_emails\", True):\n",
    "        text = re.sub(r\"\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b\", \" \", text)\n",
    "\n",
    "    if CONFIG.get(\"preprocess\", {}).get(\"remove_boilerplate\", True):\n",
    "        lines = [line.strip() for line in re.split(r\"[\\r\\n]+\", text) if line.strip()]\n",
    "        lines = [line for line in lines if not _is_boilerplate_line(line)]\n",
    "        text = \" \".join(lines)\n",
    "\n",
    "        # Trim disclaimers/signatures only when they appear near the tail.\n",
    "        text = _trim_trailing_boilerplate(text, DISCLAIMER_MARKER_RE, min_ratio=0.55)\n",
    "        text = _trim_trailing_boilerplate(text, SIGNOFF_MARKER_RE, min_ratio=0.60)\n",
    "\n",
    "    text = re.sub(r\"[^\\w\\s\\.,:;!?\\-/]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    if CONFIG.get(\"preprocess\", {}).get(\"lowercase\", False):\n",
    "        text = text.lower()\n",
    "    return text\n",
    "\n",
    "df_raw[\"ticket_text\"] = (\n",
    "    df_raw[available_text_cols]\n",
    "    .fillna(\"\")\n",
    "    .astype(str)\n",
    "    .agg(\" \".join, axis=1)\n",
    "    .map(normalize_ticket_text)\n",
    ")\n",
    "\n",
    "df_raw = df_raw[df_raw[\"ticket_text\"].str.len() > 0].copy()\n",
    "print(f\"Rows after text cleaning: {len(df_raw)}\")\n",
    "display(df_raw[[CONFIG[\"id_column\"], \"ticket_text\"]].head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3c19e73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[department] rows after label cleaning: 3000\n",
      "[department] num classes: 10\n",
      "[department] mapping saved: /Users/rawadyared/NLP-IT-Ticket_Triage/results/mappings/department_label_mapping.json\n",
      "[urgency] rows after label cleaning: 3000\n",
      "[urgency] num classes: 3\n",
      "[urgency] mapping saved: /Users/rawadyared/NLP-IT-Ticket_Triage/results/mappings/urgency_label_mapping.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Label cleaning + encoding\n",
    "def _standardize_label(label: str, task_name: str) -> str:\n",
    "    cleaned = re.sub(r\"\\s+\", \" \", str(label)).strip()\n",
    "    if cleaned == \"\":\n",
    "        return cleaned\n",
    "\n",
    "    if task_name == \"urgency\":\n",
    "        alias_map = {\n",
    "            k.casefold(): v\n",
    "            for k, v in CONFIG.get(\"label_standardization\", {}).get(\"urgency_aliases\", {}).items()\n",
    "        }\n",
    "        normalized = cleaned.casefold()\n",
    "        return alias_map.get(normalized, normalized)\n",
    "\n",
    "    alias_map = {\n",
    "        k.casefold(): v\n",
    "        for k, v in CONFIG.get(\"label_standardization\", {}).get(\"department_aliases\", {}).items()\n",
    "    }\n",
    "    normalized = cleaned.casefold()\n",
    "    return alias_map.get(normalized, cleaned)\n",
    "\n",
    "def clean_and_encode_labels(df: pd.DataFrame, label_col: str, task_name: str, mappings_dir: Path):\n",
    "    if label_col not in df.columns:\n",
    "        raise KeyError(f\"Label column '{label_col}' not found for task '{task_name}'.\")\n",
    "\n",
    "    task_df = df.copy()\n",
    "    task_df = task_df[task_df[label_col].notna()].copy()\n",
    "    task_df[label_col] = task_df[label_col].astype(str).map(lambda x: _standardize_label(x, task_name))\n",
    "    task_df = task_df[task_df[label_col] != \"\"].copy()\n",
    "\n",
    "    label_values = sorted(task_df[label_col].unique().tolist())\n",
    "    label2id = {label: idx for idx, label in enumerate(label_values)}\n",
    "    id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "    task_df[\"label_text\"] = task_df[label_col]\n",
    "    task_df[\"label\"] = task_df[\"label_text\"].map(label2id).astype(int)\n",
    "\n",
    "    mapping_payload = {\n",
    "        \"task\": task_name,\n",
    "        \"label_column\": label_col,\n",
    "        \"label2id\": label2id,\n",
    "        \"id2label\": {str(k): v for k, v in id2label.items()},\n",
    "    }\n",
    "    mapping_path = mappings_dir / f\"{task_name}_label_mapping.json\"\n",
    "    with open(mapping_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(mapping_payload, f, ensure_ascii=True, indent=2)\n",
    "\n",
    "    print(f\"[{task_name}] rows after label cleaning: {len(task_df)}\")\n",
    "    print(f\"[{task_name}] num classes: {len(label2id)}\")\n",
    "    print(f\"[{task_name}] mapping saved: {mapping_path}\")\n",
    "    return task_df, label2id, id2label\n",
    "\n",
    "dept_df, dept_label2id, dept_id2label = clean_and_encode_labels(\n",
    "    df=df_raw,\n",
    "    label_col=CONFIG[\"label_columns\"][\"department\"],\n",
    "    task_name=\"department\",\n",
    "    mappings_dir=MAPPINGS_DIR,\n",
    ")\n",
    "\n",
    "urgency_df, urgency_label2id, urgency_id2label = clean_and_encode_labels(\n",
    "    df=df_raw,\n",
    "    label_col=CONFIG[\"label_columns\"][\"urgency\"],\n",
    "    task_name=\"urgency\",\n",
    "    mappings_dir=MAPPINGS_DIR,\n",
    ")\n",
    "\n",
    "TASK_DATA = {\n",
    "    \"department\": {\n",
    "        \"df\": dept_df,\n",
    "        \"label2id\": dept_label2id,\n",
    "        \"id2label\": dept_id2label,\n",
    "    },\n",
    "    \"urgency\": {\n",
    "        \"df\": urgency_df,\n",
    "        \"label2id\": urgency_label2id,\n",
    "        \"id2label\": urgency_id2label,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "275b9022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[department] train: n=2400\n",
      "label_text\n",
      "Technical Support                  29.04\n",
      "Product Support                    18.67\n",
      "Customer Service                   15.12\n",
      "IT Support                         11.79\n",
      "Billing and Payments               10.21\n",
      "Returns and Exchanges               4.92\n",
      "Service Outages and Maintenance     3.92\n",
      "Sales and Pre-Sales                 3.00\n",
      "Human Resources                     1.92\n",
      "General Inquiry                     1.42\n",
      "\n",
      "[department] val: n=300\n",
      "label_text\n",
      "Technical Support                  29.00\n",
      "Product Support                    18.67\n",
      "Customer Service                   15.33\n",
      "IT Support                         11.67\n",
      "Billing and Payments               10.33\n",
      "Returns and Exchanges               5.00\n",
      "Service Outages and Maintenance     4.00\n",
      "Sales and Pre-Sales                 3.00\n",
      "Human Resources                     1.67\n",
      "General Inquiry                     1.33\n",
      "\n",
      "[department] test: n=300\n",
      "label_text\n",
      "Technical Support                  29.00\n",
      "Product Support                    18.67\n",
      "Customer Service                   15.00\n",
      "IT Support                         12.00\n",
      "Billing and Payments               10.00\n",
      "Returns and Exchanges               5.00\n",
      "Service Outages and Maintenance     3.67\n",
      "Sales and Pre-Sales                 3.00\n",
      "Human Resources                     2.00\n",
      "General Inquiry                     1.67\n",
      "\n",
      "[urgency] train: n=2400\n",
      "label_text\n",
      "medium    40.88\n",
      "high      38.88\n",
      "low       20.25\n",
      "\n",
      "[urgency] val: n=300\n",
      "label_text\n",
      "medium    41.00\n",
      "high      38.67\n",
      "low       20.33\n",
      "\n",
      "[urgency] test: n=300\n",
      "label_text\n",
      "medium    41.0\n",
      "high      39.0\n",
      "low       20.0\n",
      "\n",
      "Split generation complete for tasks: ['department', 'urgency']\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Train/Val/Test split (stratified) + leakage checks\n",
    "def stratified_split_with_leakage_guard(\n",
    "    task_df: pd.DataFrame,\n",
    "    id_col: str,\n",
    "    seed: int,\n",
    "    train_size: float,\n",
    "    val_size: float,\n",
    "    test_size: float,\n",
    "):\n",
    "    if not np.isclose(train_size + val_size + test_size, 1.0):\n",
    "        raise ValueError(\"Split sizes must sum to 1.0\")\n",
    "\n",
    "    working_df = task_df.copy()\n",
    "    if id_col not in working_df.columns:\n",
    "        working_df[id_col] = np.arange(len(working_df))\n",
    "\n",
    "    id_label = working_df[[id_col, \"label\"]].drop_duplicates()\n",
    "    label_per_id = id_label.groupby(id_col)[\"label\"].nunique()\n",
    "    if (label_per_id > 1).any():\n",
    "        raise ValueError(\"A ticket ID maps to multiple labels; cannot guarantee leakage-free split.\")\n",
    "\n",
    "    id_frame = id_label.drop_duplicates(subset=[id_col]).copy()\n",
    "    y = id_frame[\"label\"]\n",
    "    stratify_1 = y if y.value_counts().min() >= 2 else None\n",
    "\n",
    "    train_ids, temp_ids = train_test_split(\n",
    "        id_frame[id_col],\n",
    "        test_size=(1.0 - train_size),\n",
    "        random_state=seed,\n",
    "        stratify=stratify_1,\n",
    "    )\n",
    "\n",
    "    temp_frame = id_frame[id_frame[id_col].isin(temp_ids)].copy()\n",
    "    y_temp = temp_frame[\"label\"]\n",
    "    stratify_2 = y_temp if y_temp.value_counts().min() >= 2 else None\n",
    "    rel_test_size = test_size / (val_size + test_size)\n",
    "\n",
    "    val_ids, test_ids = train_test_split(\n",
    "        temp_frame[id_col],\n",
    "        test_size=rel_test_size,\n",
    "        random_state=seed,\n",
    "        stratify=stratify_2,\n",
    "    )\n",
    "\n",
    "    splits = {\n",
    "        \"train\": working_df[working_df[id_col].isin(set(train_ids))].reset_index(drop=True),\n",
    "        \"val\": working_df[working_df[id_col].isin(set(val_ids))].reset_index(drop=True),\n",
    "        \"test\": working_df[working_df[id_col].isin(set(test_ids))].reset_index(drop=True),\n",
    "    }\n",
    "\n",
    "    train_set = set(splits[\"train\"][id_col].tolist())\n",
    "    val_set = set(splits[\"val\"][id_col].tolist())\n",
    "    test_set = set(splits[\"test\"][id_col].tolist())\n",
    "    assert train_set.isdisjoint(val_set)\n",
    "    assert train_set.isdisjoint(test_set)\n",
    "    assert val_set.isdisjoint(test_set)\n",
    "\n",
    "    return splits\n",
    "\n",
    "def print_split_distribution(task_name: str, splits: dict):\n",
    "    for split_name, split_df in splits.items():\n",
    "        print(f\"\\n[{task_name}] {split_name}: n={len(split_df)}\")\n",
    "        dist = split_df[\"label_text\"].value_counts(normalize=True).mul(100).round(2)\n",
    "        print(dist.to_string())\n",
    "\n",
    "SPLITS = {}\n",
    "for task_name, task_info in TASK_DATA.items():\n",
    "    splits = stratified_split_with_leakage_guard(\n",
    "        task_df=task_info[\"df\"],\n",
    "        id_col=CONFIG[\"id_column\"],\n",
    "        seed=CONFIG[\"seed\"],\n",
    "        train_size=CONFIG[\"split\"][\"train_size\"],\n",
    "        val_size=CONFIG[\"split\"][\"val_size\"],\n",
    "        test_size=CONFIG[\"split\"][\"test_size\"],\n",
    "    )\n",
    "    SPLITS[task_name] = splits\n",
    "    print_split_distribution(task_name, splits)\n",
    "\n",
    "print(\"\\nSplit generation complete for tasks:\", list(SPLITS.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8213468f",
   "metadata": {},
   "source": [
    "## Baseline Plan (Justification)\n",
    "\n",
    "Before transformer fine-tuning, we establish classical NLP baselines using **TF-IDF + Logistic Regression**.\n",
    "\n",
    "- Provides a transparent performance floor for both targets.\n",
    "- Validates that improvements from transformers are meaningful.\n",
    "- Keeps evaluation consistent via accuracy, macro F1, and per-class metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f4e08a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[department][val] accuracy=0.3900 macro_f1=0.3225 weighted_f1=0.3798\n",
      "[department][test] accuracy=0.3433 macro_f1=0.2462 weighted_f1=0.3465\n",
      "Saved baseline metrics: /Users/rawadyared/NLP-IT-Ticket_Triage/results/baselines/department_baseline_metrics.json\n",
      "[urgency][val] accuracy=0.5133 macro_f1=0.4815 weighted_f1=0.5103\n",
      "[urgency][test] accuracy=0.4467 macro_f1=0.4039 weighted_f1=0.4448\n",
      "Saved baseline metrics: /Users/rawadyared/NLP-IT-Ticket_Triage/results/baselines/urgency_baseline_metrics.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Baseline (TF-IDF + Logistic Regression)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "BASELINE_DIR = RESULTS_DIR / \"baselines\"\n",
    "BASELINE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def run_tfidf_logreg_baseline(task_name: str, splits: dict, max_features: int = 80000):\n",
    "    train_df = splits[\"train\"]\n",
    "    val_df = splits[\"val\"]\n",
    "    test_df = splits[\"test\"]\n",
    "\n",
    "    pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\n",
    "                \"tfidf\",\n",
    "                TfidfVectorizer(\n",
    "                    lowercase=True,\n",
    "                    ngram_range=(1, 2),\n",
    "                    max_features=max_features,\n",
    "                    min_df=2,\n",
    "                ),\n",
    "            ),\n",
    "            (\n",
    "                \"clf\",\n",
    "                LogisticRegression(\n",
    "                    max_iter=1000,\n",
    "                    class_weight=\"balanced\",\n",
    "                    random_state=CONFIG[\"seed\"],\n",
    "                    n_jobs=None,\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    pipeline.fit(train_df[\"ticket_text\"], train_df[\"label\"])\n",
    "\n",
    "    metrics_by_split = {}\n",
    "    for split_name, split_df in [(\"val\", val_df), (\"test\", test_df)]:\n",
    "        y_true = split_df[\"label\"].values\n",
    "        y_pred = pipeline.predict(split_df[\"ticket_text\"])\n",
    "\n",
    "        split_metrics = {\n",
    "            \"accuracy\": float(accuracy_score(y_true, y_pred)),\n",
    "            \"macro_f1\": float(f1_score(y_true, y_pred, average=\"macro\", zero_division=0)),\n",
    "            \"weighted_f1\": float(f1_score(y_true, y_pred, average=\"weighted\", zero_division=0)),\n",
    "            \"classification_report\": classification_report(y_true, y_pred, output_dict=True, zero_division=0),\n",
    "            \"confusion_matrix\": confusion_matrix(y_true, y_pred).tolist(),\n",
    "        }\n",
    "        metrics_by_split[split_name] = split_metrics\n",
    "\n",
    "        print(\n",
    "            f\"[{task_name}][{split_name}] \"\n",
    "            f\"accuracy={split_metrics['accuracy']:.4f} \"\n",
    "            f\"macro_f1={split_metrics['macro_f1']:.4f} \"\n",
    "            f\"weighted_f1={split_metrics['weighted_f1']:.4f}\"\n",
    "        )\n",
    "\n",
    "    payload = {\n",
    "        \"task\": task_name,\n",
    "        \"model\": \"tfidf_logistic_regression\",\n",
    "        \"config\": {\n",
    "            \"max_features\": max_features,\n",
    "            \"ngram_range\": [1, 2],\n",
    "            \"min_df\": 2,\n",
    "            \"class_weight\": \"balanced\",\n",
    "            \"seed\": CONFIG[\"seed\"],\n",
    "        },\n",
    "        \"metrics\": metrics_by_split,\n",
    "    }\n",
    "\n",
    "    out_path = BASELINE_DIR / f\"{task_name}_baseline_metrics.json\"\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, ensure_ascii=True, indent=2)\n",
    "    print(f\"Saved baseline metrics: {out_path}\")\n",
    "\n",
    "    return pipeline, payload\n",
    "\n",
    "if \"SPLITS\" not in globals() or not isinstance(SPLITS, dict) or len(SPLITS) == 0:\n",
    "    raise RuntimeError(\"SPLITS not found. Run notebook cells in order through Cell 9 (Train/Val/Test split), then run this baseline cell.\")\n",
    "\n",
    "BASELINE_MODELS = {}\n",
    "BASELINE_RESULTS = {}\n",
    "\n",
    "for task_name in [\"department\", \"urgency\"]:\n",
    "    model, result = run_tfidf_logreg_baseline(task_name=task_name, splits=SPLITS[task_name])\n",
    "    BASELINE_MODELS[task_name] = model\n",
    "    BASELINE_RESULTS[task_name] = result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ad80e5",
   "metadata": {},
   "source": [
    "## Transformer Plan (Multiple Approaches)\n",
    "\n",
    "We compare multiple transformer backbones for **department routing** and select the best on validation macro F1.\n",
    "\n",
    "- Candidate A: `distilroberta-base` (faster)\n",
    "- Candidate B: `bert-base-uncased` (strong baseline)\n",
    "- Selection criterion: best validation macro F1, then test on held-out set\n",
    "\n",
    "Note: Hugging Face model/tokenizer files are downloaded on first use if not cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2db996b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Unnamed: 0', 'ticket_text', 'label', 'label_text'],\n",
      "        num_rows: 2400\n",
      "    })\n",
      "    val: Dataset({\n",
      "        features: ['Unnamed: 0', 'ticket_text', 'label', 'label_text'],\n",
      "        num_rows: 300\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['Unnamed: 0', 'ticket_text', 'label', 'label_text'],\n",
      "        num_rows: 300\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Convert to HuggingFace Dataset (department)\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "DEPT_SPLITS = SPLITS[\"department\"]\n",
    "DEPT_LABEL2ID = TASK_DATA[\"department\"][\"label2id\"]\n",
    "DEPT_ID2LABEL = TASK_DATA[\"department\"][\"id2label\"]\n",
    "\n",
    "def to_hf_dataset(split_df: pd.DataFrame, id_col: str) -> Dataset:\n",
    "    keep_cols = [col for col in [id_col, \"ticket_text\", \"label\", \"label_text\"] if col in split_df.columns]\n",
    "    export_df = split_df[keep_cols].copy()\n",
    "    return Dataset.from_pandas(export_df, preserve_index=False)\n",
    "\n",
    "dept_hf_raw = DatasetDict(\n",
    "    {\n",
    "        \"train\": to_hf_dataset(DEPT_SPLITS[\"train\"], CONFIG[\"id_column\"]),\n",
    "        \"val\": to_hf_dataset(DEPT_SPLITS[\"val\"], CONFIG[\"id_column\"]),\n",
    "        \"test\": to_hf_dataset(DEPT_SPLITS[\"test\"], CONFIG[\"id_column\"]),\n",
    "    }\n",
    ")\n",
    "\n",
    "print(dept_hf_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "612f4101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer function ready. First tokenizer load will download files if not cached.\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: Tokenizer + tokenize function\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "def tokenize_department_splits(tokenizer):\n",
    "    text_col = \"ticket_text\"\n",
    "    max_len = CONFIG[\"train\"][\"max_length\"]\n",
    "\n",
    "    def _tokenize(batch):\n",
    "        return tokenizer(batch[text_col], truncation=True, padding=\"max_length\", max_length=max_len)\n",
    "\n",
    "    remove_cols = [c for c in dept_hf_raw[\"train\"].column_names if c != \"label\"]\n",
    "    tokenized = dept_hf_raw.map(_tokenize, batched=True, remove_columns=remove_cols)\n",
    "    tokenized.set_format(type=\"torch\")\n",
    "    return tokenized\n",
    "\n",
    "print(\"Tokenizer function ready. First tokenizer load will download files if not cached.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "351006cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": float(accuracy_score(labels, preds)),\n",
    "        \"macro_f1\": float(f1_score(labels, preds, average=\"macro\", zero_division=0)),\n",
    "        \"weighted_f1\": float(f1_score(labels, preds, average=\"weighted\", zero_division=0)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c26126c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Model init\n",
    "def init_model(model_name: str):\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    print(\"If this is the first run, Hugging Face weights will be downloaded.\")\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=len(DEPT_LABEL2ID),\n",
    "        id2label=DEPT_ID2LABEL,\n",
    "        label2id=DEPT_LABEL2ID,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6f3b7631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: Trainer setup\n",
    "def build_trainer(model_name: str, tokenized_ds: DatasetDict, tokenizer, run_dir: Path, num_train_epochs: int, learning_rate: float) -> Trainer:\n",
    "    model = init_model(model_name)\n",
    "    data_collator = DataCollatorWithPadding(\n",
    "        tokenizer=tokenizer,\n",
    "        pad_to_multiple_of=8 if torch.cuda.is_available() else None,\n",
    "    )\n",
    "\n",
    "    training_args_kwargs = {\n",
    "        \"output_dir\": str(run_dir),\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"per_device_train_batch_size\": CONFIG[\"train\"][\"batch_size\"],\n",
    "        \"per_device_eval_batch_size\": CONFIG[\"train\"][\"batch_size\"],\n",
    "        \"num_train_epochs\": num_train_epochs,\n",
    "        \"weight_decay\": CONFIG[\"train\"][\"weight_decay\"],\n",
    "        \"warmup_ratio\": CONFIG[\"train\"][\"warmup_ratio\"],\n",
    "        \"save_strategy\": \"epoch\",\n",
    "        \"load_best_model_at_end\": True,\n",
    "        \"metric_for_best_model\": \"macro_f1\",\n",
    "        \"greater_is_better\": True,\n",
    "        \"save_total_limit\": 2,\n",
    "        \"fp16\": torch.cuda.is_available(),\n",
    "        \"logging_steps\": 50,\n",
    "        \"report_to\": [],\n",
    "        \"seed\": CONFIG[\"seed\"],\n",
    "    }\n",
    "    strategy_key = \"eval_strategy\" if \"eval_strategy\" in TrainingArguments.__init__.__code__.co_varnames else \"evaluation_strategy\"\n",
    "    training_args_kwargs[strategy_key] = \"epoch\"\n",
    "    args = TrainingArguments(**training_args_kwargs)\n",
    "\n",
    "    trainer_kwargs = {\n",
    "        \"model\": model,\n",
    "        \"args\": args,\n",
    "        \"train_dataset\": tokenized_ds[\"train\"],\n",
    "        \"eval_dataset\": tokenized_ds[\"val\"],\n",
    "        \"data_collator\": data_collator,\n",
    "        \"compute_metrics\": compute_metrics,\n",
    "        \"callbacks\": [EarlyStoppingCallback(early_stopping_patience=CONFIG[\"train\"][\"early_stopping_patience\"])],\n",
    "    }\n",
    "    processing_key = \"processing_class\" if \"processing_class\" in Trainer.__init__.__code__.co_varnames else \"tokenizer\"\n",
    "    trainer_kwargs[processing_key] = tokenizer\n",
    "    trainer = Trainer(**trainer_kwargs)\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a4b71029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Department] Trial 1/3: distilroberta-base | lr=1e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c51b1c9bd7504279bcdf771def81848e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a356f2d2d3645178658856b2aad70c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f1844723ec64cbc9aa8e94a00993aa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: distilroberta-base\n",
      "If this is the first run, Hugging Face weights will be downloaded.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='900' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [900/900 03:33, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Weighted F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.851100</td>\n",
       "      <td>1.751816</td>\n",
       "      <td>0.383333</td>\n",
       "      <td>0.160861</td>\n",
       "      <td>0.283588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.756200</td>\n",
       "      <td>1.651964</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.180795</td>\n",
       "      <td>0.302085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.633000</td>\n",
       "      <td>1.622141</td>\n",
       "      <td>0.413333</td>\n",
       "      <td>0.247082</td>\n",
       "      <td>0.339543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation macro F1: 0.2471\n",
      "\n",
      "[Department] Trial 2/3: distilroberta-base | lr=2e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81fdae849f8b41f0b703b2b1474813e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c9eda811f294fa1a3704093e59007f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5f363fe50e7458caf90adf049ca45a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: distilroberta-base\n",
      "If this is the first run, Hugging Face weights will be downloaded.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='900' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [900/900 03:30, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Weighted F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.793200</td>\n",
       "      <td>1.648134</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.185063</td>\n",
       "      <td>0.327233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.702200</td>\n",
       "      <td>1.593771</td>\n",
       "      <td>0.376667</td>\n",
       "      <td>0.226634</td>\n",
       "      <td>0.317421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.530600</td>\n",
       "      <td>1.565067</td>\n",
       "      <td>0.393333</td>\n",
       "      <td>0.236976</td>\n",
       "      <td>0.336591</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation macro F1: 0.2370\n",
      "\n",
      "[Department] Trial 3/3: distilroberta-base | lr=3e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd4bdbcd8cf840abae47e3c178ec26db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47eccd71cd234334a1dd4d0fa05e5198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ee1f65328064fe9b2626f1ef6af3937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: distilroberta-base\n",
      "If this is the first run, Hugging Face weights will be downloaded.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='900' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [900/900 03:31, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Weighted F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.797800</td>\n",
       "      <td>1.647383</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>0.160228</td>\n",
       "      <td>0.280430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.694900</td>\n",
       "      <td>1.578655</td>\n",
       "      <td>0.393333</td>\n",
       "      <td>0.234348</td>\n",
       "      <td>0.325253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.492800</td>\n",
       "      <td>1.541799</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>0.235523</td>\n",
       "      <td>0.330018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation macro F1: 0.2355\n",
      "\n",
      "Candidate comparison complete.\n",
      "        model_name  learning_rate  candidate_epochs  val_accuracy  val_macro_f1  val_weighted_f1\n",
      "distilroberta-base        0.00001                 3      0.413333      0.247082         0.339543\n",
      "distilroberta-base        0.00002                 3      0.393333      0.236976         0.336591\n",
      "distilroberta-base        0.00003                 3      0.390000      0.235523         0.330018\n",
      "Saved best department model to: /Users/rawadyared/NLP-IT-Ticket_Triage/models/department_model/best\n",
      "Saved model-selection report: /Users/rawadyared/NLP-IT-Ticket_Triage/results/department_model_selection.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 18: Train department classifier (candidate comparison + best model save)\n",
    "DEPT_MODELS_DIR = MODELS_DIR / \"department_model\"\n",
    "DEPT_MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "candidate_models = (\n",
    "    CONFIG[\"candidate_models\"][\"department\"]\n",
    "    if CONFIG[\"experiment\"][\"run_candidate_search\"]\n",
    "    else [CONFIG[\"model_names\"][\"department\"]]\n",
    ")\n",
    "\n",
    "learning_rates = (\n",
    "    CONFIG[\"train\"].get(\"learning_rates\", [CONFIG[\"train\"][\"learning_rate\"]])\n",
    "    if CONFIG[\"experiment\"].get(\"tune_learning_rate\", True)\n",
    "    else [CONFIG[\"train\"][\"learning_rate\"]]\n",
    ")\n",
    "\n",
    "candidate_epochs = CONFIG[\"experiment\"][\"candidate_epochs\"]\n",
    "full_epochs = CONFIG[\"train\"][\"epochs\"]\n",
    "\n",
    "DEPT_EXPERIMENTS = []\n",
    "DEPT_BEST = None\n",
    "DEPT_BEST_TRAINER = None\n",
    "DEPT_BEST_TOKENIZER = None\n",
    "DEPT_BEST_TOKENIZED_DS = None\n",
    "\n",
    "run_idx = 0\n",
    "for model_name in candidate_models:\n",
    "    for lr in learning_rates:\n",
    "        run_idx += 1\n",
    "        safe_name = model_name.replace(\"/\", \"_\")\n",
    "        lr_tag = str(lr).replace(\".\", \"p\")\n",
    "        run_dir = DEPT_MODELS_DIR / f\"candidate_{run_idx}_{safe_name}_lr{lr_tag}\"\n",
    "        run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        print(\n",
    "            f\"\\n[Department] Trial {run_idx}/{len(candidate_models) * len(learning_rates)}: \"\n",
    "            f\"{model_name} | lr={lr}\"\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "        tokenized_ds = tokenize_department_splits(tokenizer)\n",
    "\n",
    "        trainer = build_trainer(\n",
    "            model_name=model_name,\n",
    "            tokenized_ds=tokenized_ds,\n",
    "            tokenizer=tokenizer,\n",
    "            run_dir=run_dir,\n",
    "            num_train_epochs=candidate_epochs,\n",
    "            learning_rate=lr,\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        val_metrics = trainer.evaluate(tokenized_ds[\"val\"])\n",
    "        val_macro_f1 = float(val_metrics.get(\"eval_macro_f1\", -1.0))\n",
    "\n",
    "        experiment_row = {\n",
    "            \"model_name\": model_name,\n",
    "            \"learning_rate\": lr,\n",
    "            \"candidate_epochs\": candidate_epochs,\n",
    "            \"val_accuracy\": float(val_metrics.get(\"eval_accuracy\", 0.0)),\n",
    "            \"val_macro_f1\": val_macro_f1,\n",
    "            \"val_weighted_f1\": float(val_metrics.get(\"eval_weighted_f1\", 0.0)),\n",
    "        }\n",
    "        DEPT_EXPERIMENTS.append(experiment_row)\n",
    "        print(f\"Validation macro F1: {val_macro_f1:.4f}\")\n",
    "\n",
    "        if (DEPT_BEST is None) or (val_macro_f1 > DEPT_BEST[\"val_macro_f1\"]):\n",
    "            DEPT_BEST = experiment_row\n",
    "            DEPT_BEST_TRAINER = trainer\n",
    "            DEPT_BEST_TOKENIZER = tokenizer\n",
    "            DEPT_BEST_TOKENIZED_DS = tokenized_ds\n",
    "\n",
    "print(\"\\nCandidate comparison complete.\")\n",
    "print(pd.DataFrame(DEPT_EXPERIMENTS).sort_values(\"val_macro_f1\", ascending=False).to_string(index=False))\n",
    "\n",
    "best_model_name = DEPT_BEST[\"model_name\"]\n",
    "best_learning_rate = DEPT_BEST[\"learning_rate\"]\n",
    "best_model_dir = DEPT_MODELS_DIR / \"best\"\n",
    "best_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Optional full retrain on best candidate with configured epochs.\n",
    "if full_epochs > candidate_epochs:\n",
    "    print(\n",
    "        f\"\\nRetraining best candidate ({best_model_name}, lr={best_learning_rate}) \"\n",
    "        f\"for full epochs: {full_epochs}\"\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(best_model_name, use_fast=True)\n",
    "    tokenized_ds = tokenize_department_splits(tokenizer)\n",
    "    trainer = build_trainer(\n",
    "        model_name=best_model_name,\n",
    "        tokenized_ds=tokenized_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        run_dir=best_model_dir,\n",
    "        num_train_epochs=full_epochs,\n",
    "        learning_rate=best_learning_rate,\n",
    "    )\n",
    "    trainer.train()\n",
    "    DEPT_BEST_TRAINER = trainer\n",
    "    DEPT_BEST_TOKENIZER = tokenizer\n",
    "    DEPT_BEST_TOKENIZED_DS = tokenized_ds\n",
    "\n",
    "DEPT_BEST_TRAINER.save_model(str(best_model_dir))\n",
    "DEPT_BEST_TOKENIZER.save_pretrained(str(best_model_dir))\n",
    "print(f\"Saved best department model to: {best_model_dir}\")\n",
    "\n",
    "dept_experiment_path = RESULTS_DIR / \"department_model_selection.json\"\n",
    "with open(dept_experiment_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"selected_model\": best_model_name,\n",
    "            \"selected_learning_rate\": best_learning_rate,\n",
    "            \"candidate_results\": DEPT_EXPERIMENTS,\n",
    "            \"learning_rates\": learning_rates,\n",
    "            \"candidate_epochs\": candidate_epochs,\n",
    "            \"full_epochs\": full_epochs,\n",
    "        },\n",
    "        f,\n",
    "        ensure_ascii=True,\n",
    "        indent=2,\n",
    "    )\n",
    "print(f\"Saved model-selection report: {dept_experiment_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4a681816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Department transformer test -> accuracy=0.3867, macro_f1=0.2001, weighted_f1=0.3219\n",
      "Saved test metrics: /Users/rawadyared/NLP-IT-Ticket_Triage/results/department_transformer_metrics.json\n",
      "Saved test predictions: /Users/rawadyared/NLP-IT-Ticket_Triage/results/department_test_predictions.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ticket_text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_text</th>\n",
       "      <th>pred_label_id</th>\n",
       "      <th>pred_label</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2966</td>\n",
       "      <td>Our marketing agency faced several hardware and software failures, which affected our digital campaigns. The possible root cause was compatibility issues. T...</td>\n",
       "      <td>9</td>\n",
       "      <td>Technical Support</td>\n",
       "      <td>9</td>\n",
       "      <td>Technical Support</td>\n",
       "      <td>0.458194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29612</td>\n",
       "      <td>Greetings Customer Support,I am having difficulties in updating Norton Antivirus Plus, and I need assistance. I have attempted various troubleshooting metho...</td>\n",
       "      <td>9</td>\n",
       "      <td>Technical Support</td>\n",
       "      <td>9</td>\n",
       "      <td>Technical Support</td>\n",
       "      <td>0.393538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29395</td>\n",
       "      <td>Problems encountered during Kaspersky installation. Require help.</td>\n",
       "      <td>4</td>\n",
       "      <td>IT Support</td>\n",
       "      <td>9</td>\n",
       "      <td>Technical Support</td>\n",
       "      <td>0.442360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19002</td>\n",
       "      <td>Customer Support, I am seeking detailed guidance on integrating Airtable project management software into my workflow. Could you provide comprehensive instr...</td>\n",
       "      <td>1</td>\n",
       "      <td>Customer Service</td>\n",
       "      <td>9</td>\n",
       "      <td>Technical Support</td>\n",
       "      <td>0.279409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24976</td>\n",
       "      <td>We have noticed a decline in engagement for our digital campaigns. We have reviewed user feedback and adjusted our targeting, but we need assistance in pinp...</td>\n",
       "      <td>5</td>\n",
       "      <td>Product Support</td>\n",
       "      <td>5</td>\n",
       "      <td>Product Support</td>\n",
       "      <td>0.316021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  \\\n",
       "0        2966   \n",
       "1       29612   \n",
       "2       29395   \n",
       "3       19002   \n",
       "4       24976   \n",
       "\n",
       "                                                                                                                                                       ticket_text  \\\n",
       "0  Our marketing agency faced several hardware and software failures, which affected our digital campaigns. The possible root cause was compatibility issues. T...   \n",
       "1  Greetings Customer Support,I am having difficulties in updating Norton Antivirus Plus, and I need assistance. I have attempted various troubleshooting metho...   \n",
       "2                                                                                                Problems encountered during Kaspersky installation. Require help.   \n",
       "3  Customer Support, I am seeking detailed guidance on integrating Airtable project management software into my workflow. Could you provide comprehensive instr...   \n",
       "4  We have noticed a decline in engagement for our digital campaigns. We have reviewed user feedback and adjusted our targeting, but we need assistance in pinp...   \n",
       "\n",
       "   label         label_text  pred_label_id         pred_label  confidence  \n",
       "0      9  Technical Support              9  Technical Support    0.458194  \n",
       "1      9  Technical Support              9  Technical Support    0.393538  \n",
       "2      4         IT Support              9  Technical Support    0.442360  \n",
       "3      1   Customer Service              9  Technical Support    0.279409  \n",
       "4      5    Product Support              5    Product Support    0.316021  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 19: Evaluate department classifier (test)\n",
    "dept_test_output = DEPT_BEST_TRAINER.predict(DEPT_BEST_TOKENIZED_DS[\"test\"])\n",
    "dept_test_logits = dept_test_output.predictions\n",
    "dept_test_probs = torch.softmax(torch.tensor(dept_test_logits), dim=-1).cpu().numpy()\n",
    "\n",
    "y_true = DEPT_SPLITS[\"test\"][\"label\"].to_numpy()\n",
    "y_pred = dept_test_probs.argmax(axis=1)\n",
    "conf = dept_test_probs.max(axis=1)\n",
    "\n",
    "dept_test_metrics = {\n",
    "    \"accuracy\": float(accuracy_score(y_true, y_pred)),\n",
    "    \"macro_f1\": float(f1_score(y_true, y_pred, average=\"macro\", zero_division=0)),\n",
    "    \"weighted_f1\": float(f1_score(y_true, y_pred, average=\"weighted\", zero_division=0)),\n",
    "    \"classification_report\": classification_report(y_true, y_pred, output_dict=True, zero_division=0),\n",
    "    \"confusion_matrix\": confusion_matrix(y_true, y_pred).tolist(),\n",
    "}\n",
    "\n",
    "print(\n",
    "    f\"Department transformer test -> \"\n",
    "    f\"accuracy={dept_test_metrics['accuracy']:.4f}, \"\n",
    "    f\"macro_f1={dept_test_metrics['macro_f1']:.4f}, \"\n",
    "    f\"weighted_f1={dept_test_metrics['weighted_f1']:.4f}\"\n",
    ")\n",
    "\n",
    "metrics_payload = {\n",
    "    \"task\": \"department\",\n",
    "    \"selected_model\": DEPT_BEST[\"model_name\"],\n",
    "    \"selected_learning_rate\": DEPT_BEST.get(\"learning_rate\", CONFIG[\"train\"][\"learning_rate\"]),\n",
    "    \"test_metrics\": dept_test_metrics,\n",
    "}\n",
    "dept_metrics_path = RESULTS_DIR / \"department_transformer_metrics.json\"\n",
    "with open(dept_metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metrics_payload, f, ensure_ascii=True, indent=2)\n",
    "print(f\"Saved test metrics: {dept_metrics_path}\")\n",
    "\n",
    "id_col = CONFIG[\"id_column\"]\n",
    "pred_df = DEPT_SPLITS[\"test\"][[id_col, \"ticket_text\", \"label\", \"label_text\"]].copy()\n",
    "pred_df[\"pred_label_id\"] = y_pred\n",
    "pred_df[\"pred_label\"] = pred_df[\"pred_label_id\"].map(DEPT_ID2LABEL)\n",
    "pred_df[\"confidence\"] = conf\n",
    "\n",
    "pred_out_path = RESULTS_DIR / \"department_test_predictions.csv\"\n",
    "pred_df.to_csv(pred_out_path, index=False)\n",
    "print(f\"Saved test predictions: {pred_out_path}\")\n",
    "display(pred_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ddfa7c",
   "metadata": {},
   "source": [
    "## Urgency Transformer (Multiple Approaches)\n",
    "\n",
    "We now apply the same multi-approach process to urgency/priority:\n",
    "\n",
    "- Candidate A: `distilroberta-base`\n",
    "- Candidate B: `bert-base-uncased`\n",
    "- Model selection: best validation macro F1\n",
    "\n",
    "Note: Hugging Face tokenizer/model files are downloaded on first run if not cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "512509f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Urgency] Trial 1/3: distilroberta-base | lr=1e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f2d0ba35fd4b6d8fd8f8eb6563e536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ede865f1d1746b6b4228ff6203f06b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01106606f4d8498994ea317379884914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading urgency model: distilroberta-base\n",
      "If first run, Hugging Face downloads tokenizer and model weights.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='900' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [900/900 03:33, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Weighted F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.061500</td>\n",
       "      <td>1.029061</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.321703</td>\n",
       "      <td>0.383680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.003500</td>\n",
       "      <td>1.018770</td>\n",
       "      <td>0.473333</td>\n",
       "      <td>0.343722</td>\n",
       "      <td>0.411852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.978400</td>\n",
       "      <td>1.008657</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.346701</td>\n",
       "      <td>0.415546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation macro F1: 0.3467\n",
      "\n",
      "[Urgency] Trial 2/3: distilroberta-base | lr=2e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47f910ced9b3496c8d8b61c2a5f2deaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81cc878bc663430b8cc8cc486012c6e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31f584cba25e4637acc5c3c09e63595f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading urgency model: distilroberta-base\n",
      "If first run, Hugging Face downloads tokenizer and model weights.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/900 02:19 < 01:09, 4.29 it/s, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Weighted F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.054000</td>\n",
       "      <td>1.036475</td>\n",
       "      <td>0.496667</td>\n",
       "      <td>0.349901</td>\n",
       "      <td>0.419886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.005900</td>\n",
       "      <td>1.022274</td>\n",
       "      <td>0.476667</td>\n",
       "      <td>0.344995</td>\n",
       "      <td>0.413392</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation macro F1: 0.3499\n",
      "\n",
      "[Urgency] Trial 3/3: distilroberta-base | lr=3e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c35a87fccab4899993485c80001fa9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "072a3a7cbaac40f2b864f875ec337618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9511bf747d2943cbaf407ccfe277edfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading urgency model: distilroberta-base\n",
      "If first run, Hugging Face downloads tokenizer and model weights.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='900' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [900/900 03:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Weighted F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.072500</td>\n",
       "      <td>1.059424</td>\n",
       "      <td>0.386667</td>\n",
       "      <td>0.185897</td>\n",
       "      <td>0.215641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.060000</td>\n",
       "      <td>1.049227</td>\n",
       "      <td>0.446667</td>\n",
       "      <td>0.297829</td>\n",
       "      <td>0.358886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.025100</td>\n",
       "      <td>1.024814</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.335312</td>\n",
       "      <td>0.401613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation macro F1: 0.3353\n",
      "\n",
      "Urgency candidate comparison complete.\n",
      "        model_name  learning_rate  candidate_epochs  val_accuracy  val_macro_f1  val_weighted_f1\n",
      "distilroberta-base        0.00002                 3      0.496667      0.349901         0.419886\n",
      "distilroberta-base        0.00001                 3      0.480000      0.346701         0.415546\n",
      "distilroberta-base        0.00003                 3      0.460000      0.335312         0.401613\n",
      "Saved best urgency model to: /Users/rawadyared/NLP-IT-Ticket_Triage/models/urgency_model/best\n",
      "Saved urgency model-selection report: /Users/rawadyared/NLP-IT-Ticket_Triage/results/urgency_model_selection.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 20: Repeat transformer pipeline for urgency/priority (multi-approach)\n",
    "URGENCY_SPLITS = SPLITS[\"urgency\"]\n",
    "URGENCY_LABEL2ID = TASK_DATA[\"urgency\"][\"label2id\"]\n",
    "URGENCY_ID2LABEL = TASK_DATA[\"urgency\"][\"id2label\"]\n",
    "\n",
    "URGENCY_MODELS_DIR = MODELS_DIR / \"urgency_model\"\n",
    "URGENCY_MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def to_hf_dataset_urgency(split_df: pd.DataFrame, id_col: str) -> Dataset:\n",
    "    keep_cols = [col for col in [id_col, \"ticket_text\", \"label\", \"label_text\"] if col in split_df.columns]\n",
    "    return Dataset.from_pandas(split_df[keep_cols].copy(), preserve_index=False)\n",
    "\n",
    "urgency_hf_raw = DatasetDict(\n",
    "    {\n",
    "        \"train\": to_hf_dataset_urgency(URGENCY_SPLITS[\"train\"], CONFIG[\"id_column\"]),\n",
    "        \"val\": to_hf_dataset_urgency(URGENCY_SPLITS[\"val\"], CONFIG[\"id_column\"]),\n",
    "        \"test\": to_hf_dataset_urgency(URGENCY_SPLITS[\"test\"], CONFIG[\"id_column\"]),\n",
    "    }\n",
    ")\n",
    "\n",
    "def tokenize_urgency_splits(tokenizer):\n",
    "    def _tokenize(batch):\n",
    "        return tokenizer(\n",
    "            batch[\"ticket_text\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=CONFIG[\"train\"][\"max_length\"],\n",
    "        )\n",
    "\n",
    "    remove_cols = [c for c in urgency_hf_raw[\"train\"].column_names if c != \"label\"]\n",
    "    tokenized = urgency_hf_raw.map(_tokenize, batched=True, remove_columns=remove_cols)\n",
    "    tokenized.set_format(type=\"torch\")\n",
    "    return tokenized\n",
    "\n",
    "def init_urgency_model(model_name: str):\n",
    "    print(f\"Loading urgency model: {model_name}\")\n",
    "    print(\"If first run, Hugging Face downloads tokenizer and model weights.\")\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=len(URGENCY_LABEL2ID),\n",
    "        id2label=URGENCY_ID2LABEL,\n",
    "        label2id=URGENCY_LABEL2ID,\n",
    "    )\n",
    "\n",
    "def build_urgency_trainer(model_name: str, tokenized_ds: DatasetDict, tokenizer, run_dir: Path, num_train_epochs: int, learning_rate: float) -> Trainer:\n",
    "    model = init_urgency_model(model_name)\n",
    "    data_collator = DataCollatorWithPadding(\n",
    "        tokenizer=tokenizer,\n",
    "        pad_to_multiple_of=8 if torch.cuda.is_available() else None,\n",
    "    )\n",
    "\n",
    "    training_args_kwargs = {\n",
    "        \"output_dir\": str(run_dir),\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"per_device_train_batch_size\": CONFIG[\"train\"][\"batch_size\"],\n",
    "        \"per_device_eval_batch_size\": CONFIG[\"train\"][\"batch_size\"],\n",
    "        \"num_train_epochs\": num_train_epochs,\n",
    "        \"weight_decay\": CONFIG[\"train\"][\"weight_decay\"],\n",
    "        \"warmup_ratio\": CONFIG[\"train\"][\"warmup_ratio\"],\n",
    "        \"save_strategy\": \"epoch\",\n",
    "        \"load_best_model_at_end\": True,\n",
    "        \"metric_for_best_model\": \"macro_f1\",\n",
    "        \"greater_is_better\": True,\n",
    "        \"save_total_limit\": 2,\n",
    "        \"fp16\": torch.cuda.is_available(),\n",
    "        \"logging_steps\": 50,\n",
    "        \"report_to\": [],\n",
    "        \"seed\": CONFIG[\"seed\"],\n",
    "    }\n",
    "    strategy_key = \"eval_strategy\" if \"eval_strategy\" in TrainingArguments.__init__.__code__.co_varnames else \"evaluation_strategy\"\n",
    "    training_args_kwargs[strategy_key] = \"epoch\"\n",
    "    args = TrainingArguments(**training_args_kwargs)\n",
    "\n",
    "    trainer_kwargs = {\n",
    "        \"model\": model,\n",
    "        \"args\": args,\n",
    "        \"train_dataset\": tokenized_ds[\"train\"],\n",
    "        \"eval_dataset\": tokenized_ds[\"val\"],\n",
    "        \"data_collator\": data_collator,\n",
    "        \"compute_metrics\": compute_metrics,\n",
    "        \"callbacks\": [EarlyStoppingCallback(early_stopping_patience=CONFIG[\"train\"][\"early_stopping_patience\"])],\n",
    "    }\n",
    "    processing_key = \"processing_class\" if \"processing_class\" in Trainer.__init__.__code__.co_varnames else \"tokenizer\"\n",
    "    trainer_kwargs[processing_key] = tokenizer\n",
    "    return Trainer(**trainer_kwargs)\n",
    "\n",
    "urgency_candidate_models = (\n",
    "    CONFIG[\"candidate_models\"][\"urgency\"]\n",
    "    if CONFIG[\"experiment\"][\"run_candidate_search\"]\n",
    "    else [CONFIG[\"model_names\"][\"urgency\"]]\n",
    ")\n",
    "\n",
    "urgency_learning_rates = (\n",
    "    CONFIG[\"train\"].get(\"learning_rates\", [CONFIG[\"train\"][\"learning_rate\"]])\n",
    "    if CONFIG[\"experiment\"].get(\"tune_learning_rate\", True)\n",
    "    else [CONFIG[\"train\"][\"learning_rate\"]]\n",
    ")\n",
    "\n",
    "urgency_candidate_epochs = CONFIG[\"experiment\"][\"candidate_epochs\"]\n",
    "urgency_full_epochs = CONFIG[\"train\"][\"epochs\"]\n",
    "\n",
    "URGENCY_EXPERIMENTS = []\n",
    "URGENCY_BEST = None\n",
    "URGENCY_BEST_TRAINER = None\n",
    "URGENCY_BEST_TOKENIZER = None\n",
    "URGENCY_BEST_TOKENIZED_DS = None\n",
    "\n",
    "urgency_run_idx = 0\n",
    "for model_name in urgency_candidate_models:\n",
    "    for lr in urgency_learning_rates:\n",
    "        urgency_run_idx += 1\n",
    "        safe_name = model_name.replace(\"/\", \"_\")\n",
    "        lr_tag = str(lr).replace(\".\", \"p\")\n",
    "        run_dir = URGENCY_MODELS_DIR / f\"candidate_{urgency_run_idx}_{safe_name}_lr{lr_tag}\"\n",
    "        run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        print(\n",
    "            f\"\\n[Urgency] Trial {urgency_run_idx}/{len(urgency_candidate_models) * len(urgency_learning_rates)}: \"\n",
    "            f\"{model_name} | lr={lr}\"\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "        tokenized_ds = tokenize_urgency_splits(tokenizer)\n",
    "\n",
    "        trainer = build_urgency_trainer(\n",
    "            model_name=model_name,\n",
    "            tokenized_ds=tokenized_ds,\n",
    "            tokenizer=tokenizer,\n",
    "            run_dir=run_dir,\n",
    "            num_train_epochs=urgency_candidate_epochs,\n",
    "            learning_rate=lr,\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        val_metrics = trainer.evaluate(tokenized_ds[\"val\"])\n",
    "        val_macro_f1 = float(val_metrics.get(\"eval_macro_f1\", -1.0))\n",
    "\n",
    "        experiment_row = {\n",
    "            \"model_name\": model_name,\n",
    "            \"learning_rate\": lr,\n",
    "            \"candidate_epochs\": urgency_candidate_epochs,\n",
    "            \"val_accuracy\": float(val_metrics.get(\"eval_accuracy\", 0.0)),\n",
    "            \"val_macro_f1\": val_macro_f1,\n",
    "            \"val_weighted_f1\": float(val_metrics.get(\"eval_weighted_f1\", 0.0)),\n",
    "        }\n",
    "        URGENCY_EXPERIMENTS.append(experiment_row)\n",
    "        print(f\"Validation macro F1: {val_macro_f1:.4f}\")\n",
    "\n",
    "        if (URGENCY_BEST is None) or (val_macro_f1 > URGENCY_BEST[\"val_macro_f1\"]):\n",
    "            URGENCY_BEST = experiment_row\n",
    "            URGENCY_BEST_TRAINER = trainer\n",
    "            URGENCY_BEST_TOKENIZER = tokenizer\n",
    "            URGENCY_BEST_TOKENIZED_DS = tokenized_ds\n",
    "\n",
    "print(\"\\nUrgency candidate comparison complete.\")\n",
    "print(pd.DataFrame(URGENCY_EXPERIMENTS).sort_values(\"val_macro_f1\", ascending=False).to_string(index=False))\n",
    "\n",
    "urgency_best_model_name = URGENCY_BEST[\"model_name\"]\n",
    "urgency_best_learning_rate = URGENCY_BEST[\"learning_rate\"]\n",
    "urgency_best_model_dir = URGENCY_MODELS_DIR / \"best\"\n",
    "urgency_best_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if urgency_full_epochs > urgency_candidate_epochs:\n",
    "    print(\n",
    "        f\"\\nRetraining best urgency candidate ({urgency_best_model_name}, lr={urgency_best_learning_rate}) \"\n",
    "        f\"for full epochs: {urgency_full_epochs}\"\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(urgency_best_model_name, use_fast=True)\n",
    "    tokenized_ds = tokenize_urgency_splits(tokenizer)\n",
    "    trainer = build_urgency_trainer(\n",
    "        model_name=urgency_best_model_name,\n",
    "        tokenized_ds=tokenized_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        run_dir=urgency_best_model_dir,\n",
    "        num_train_epochs=urgency_full_epochs,\n",
    "        learning_rate=urgency_best_learning_rate,\n",
    "    )\n",
    "    trainer.train()\n",
    "    URGENCY_BEST_TRAINER = trainer\n",
    "    URGENCY_BEST_TOKENIZER = tokenizer\n",
    "    URGENCY_BEST_TOKENIZED_DS = tokenized_ds\n",
    "\n",
    "URGENCY_BEST_TRAINER.save_model(str(urgency_best_model_dir))\n",
    "URGENCY_BEST_TOKENIZER.save_pretrained(str(urgency_best_model_dir))\n",
    "print(f\"Saved best urgency model to: {urgency_best_model_dir}\")\n",
    "\n",
    "urgency_selection_path = RESULTS_DIR / \"urgency_model_selection.json\"\n",
    "with open(urgency_selection_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"selected_model\": urgency_best_model_name,\n",
    "            \"selected_learning_rate\": urgency_best_learning_rate,\n",
    "            \"candidate_results\": URGENCY_EXPERIMENTS,\n",
    "            \"learning_rates\": urgency_learning_rates,\n",
    "            \"candidate_epochs\": urgency_candidate_epochs,\n",
    "            \"full_epochs\": urgency_full_epochs,\n",
    "        },\n",
    "        f,\n",
    "        ensure_ascii=True,\n",
    "        indent=2,\n",
    "    )\n",
    "print(f\"Saved urgency model-selection report: {urgency_selection_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3c896f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Urgency transformer test -> accuracy=0.4467, macro_f1=0.2954, weighted_f1=0.3571\n",
      "Saved urgency test metrics: /Users/rawadyared/NLP-IT-Ticket_Triage/results/urgency_transformer_metrics.json\n",
      "Saved urgency test predictions: /Users/rawadyared/NLP-IT-Ticket_Triage/results/urgency_test_predictions.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ticket_text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_text</th>\n",
       "      <th>pred_label_id</th>\n",
       "      <th>pred_label</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27583</td>\n",
       "      <td>Dear Customer Service Team,I am contacting you to address a problem with the forecast predictions offered. These forecasts appear to be unreliable due to in...</td>\n",
       "      <td>2</td>\n",
       "      <td>medium</td>\n",
       "      <td>2</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.419524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6598</td>\n",
       "      <td>There is a risk that the medical records have been exposed due to a data leak. Outdated security measures and old software might have played a role in this.</td>\n",
       "      <td>2</td>\n",
       "      <td>medium</td>\n",
       "      <td>2</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.405263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19435</td>\n",
       "      <td>A financial firm is encountering software incompatibility problems following a recent system update. Reinstalling and updating the software may resolve the ...</td>\n",
       "      <td>2</td>\n",
       "      <td>medium</td>\n",
       "      <td>2</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.420988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14000</td>\n",
       "      <td>Dear Customer Support Team, I am writing to request updates to our data security protocols to better safeguard medical data across all applications. Given t...</td>\n",
       "      <td>1</td>\n",
       "      <td>low</td>\n",
       "      <td>2</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.422017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5858</td>\n",
       "      <td>The agency s digital marketing efforts did not enhance brand visibility; potential segmentation mistakes or targeting errors may have contributed. Efforts m...</td>\n",
       "      <td>2</td>\n",
       "      <td>medium</td>\n",
       "      <td>2</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.408694</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  \\\n",
       "0       27583   \n",
       "1        6598   \n",
       "2       19435   \n",
       "3       14000   \n",
       "4        5858   \n",
       "\n",
       "                                                                                                                                                       ticket_text  \\\n",
       "0  Dear Customer Service Team,I am contacting you to address a problem with the forecast predictions offered. These forecasts appear to be unreliable due to in...   \n",
       "1     There is a risk that the medical records have been exposed due to a data leak. Outdated security measures and old software might have played a role in this.   \n",
       "2  A financial firm is encountering software incompatibility problems following a recent system update. Reinstalling and updating the software may resolve the ...   \n",
       "3  Dear Customer Support Team, I am writing to request updates to our data security protocols to better safeguard medical data across all applications. Given t...   \n",
       "4  The agency s digital marketing efforts did not enhance brand visibility; potential segmentation mistakes or targeting errors may have contributed. Efforts m...   \n",
       "\n",
       "   label label_text  pred_label_id pred_label  confidence  \n",
       "0      2     medium              2     medium    0.419524  \n",
       "1      2     medium              2     medium    0.405263  \n",
       "2      2     medium              2     medium    0.420988  \n",
       "3      1        low              2     medium    0.422017  \n",
       "4      2     medium              2     medium    0.408694  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 20 (cont.): Evaluate urgency classifier (test)\n",
    "urgency_test_output = URGENCY_BEST_TRAINER.predict(URGENCY_BEST_TOKENIZED_DS[\"test\"])\n",
    "urgency_test_logits = urgency_test_output.predictions\n",
    "urgency_test_probs = torch.softmax(torch.tensor(urgency_test_logits), dim=-1).cpu().numpy()\n",
    "\n",
    "y_true = URGENCY_SPLITS[\"test\"][\"label\"].to_numpy()\n",
    "y_pred = urgency_test_probs.argmax(axis=1)\n",
    "conf = urgency_test_probs.max(axis=1)\n",
    "\n",
    "urgency_test_metrics = {\n",
    "    \"accuracy\": float(accuracy_score(y_true, y_pred)),\n",
    "    \"macro_f1\": float(f1_score(y_true, y_pred, average=\"macro\", zero_division=0)),\n",
    "    \"weighted_f1\": float(f1_score(y_true, y_pred, average=\"weighted\", zero_division=0)),\n",
    "    \"classification_report\": classification_report(y_true, y_pred, output_dict=True, zero_division=0),\n",
    "    \"confusion_matrix\": confusion_matrix(y_true, y_pred).tolist(),\n",
    "}\n",
    "\n",
    "print(\n",
    "    f\"Urgency transformer test -> \"\n",
    "    f\"accuracy={urgency_test_metrics['accuracy']:.4f}, \"\n",
    "    f\"macro_f1={urgency_test_metrics['macro_f1']:.4f}, \"\n",
    "    f\"weighted_f1={urgency_test_metrics['weighted_f1']:.4f}\"\n",
    ")\n",
    "\n",
    "urgency_metrics_payload = {\n",
    "    \"task\": \"urgency\",\n",
    "    \"selected_model\": URGENCY_BEST[\"model_name\"],\n",
    "    \"selected_learning_rate\": URGENCY_BEST.get(\"learning_rate\", CONFIG[\"train\"][\"learning_rate\"]),\n",
    "    \"test_metrics\": urgency_test_metrics,\n",
    "}\n",
    "urgency_metrics_path = RESULTS_DIR / \"urgency_transformer_metrics.json\"\n",
    "with open(urgency_metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(urgency_metrics_payload, f, ensure_ascii=True, indent=2)\n",
    "print(f\"Saved urgency test metrics: {urgency_metrics_path}\")\n",
    "\n",
    "id_col = CONFIG[\"id_column\"]\n",
    "urgency_pred_df = URGENCY_SPLITS[\"test\"][[id_col, \"ticket_text\", \"label\", \"label_text\"]].copy()\n",
    "urgency_pred_df[\"pred_label_id\"] = y_pred\n",
    "urgency_pred_df[\"pred_label\"] = urgency_pred_df[\"pred_label_id\"].map(URGENCY_ID2LABEL)\n",
    "urgency_pred_df[\"confidence\"] = conf\n",
    "\n",
    "urgency_pred_out_path = RESULTS_DIR / \"urgency_test_predictions.csv\"\n",
    "urgency_pred_df.to_csv(urgency_pred_out_path, index=False)\n",
    "print(f\"Saved urgency test predictions: {urgency_pred_out_path}\")\n",
    "display(urgency_pred_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9af927",
   "metadata": {},
   "source": [
    "## Tags and Summary (Prototype Inference)\n",
    "\n",
    "Approach:\n",
    "\n",
    "- **Tags**: extractive keyphrases with YAKE (lightweight, stable, CPU-friendly)\n",
    "- **Summary**: abstractive summary with `t5-small`\n",
    "\n",
    "Download note:\n",
    "\n",
    "- `t5-small` is downloaded from Hugging Face on first run if not cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4265d3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YAKE tag extractor ready.\n"
     ]
    }
   ],
   "source": [
    "# Cell 22: Tags extraction module (YAKE)\n",
    "import yake\n",
    "\n",
    "YAKE_CONFIG = {\n",
    "    \"lan\": \"en\",\n",
    "    \"n\": 3,\n",
    "    \"dedupLim\": 0.9,\n",
    "    \"dedupFunc\": \"seqm\",\n",
    "    \"windowsSize\": 1,\n",
    "}\n",
    "\n",
    "def extract_tags(text: str, top_k: int = 5):\n",
    "    clean_text = normalize_ticket_text(text)\n",
    "    if not clean_text:\n",
    "        return []\n",
    "\n",
    "    extractor = yake.KeywordExtractor(top=top_k * 2, **YAKE_CONFIG)\n",
    "    keywords = extractor.extract_keywords(clean_text)\n",
    "\n",
    "    tags = []\n",
    "    seen = set()\n",
    "    for phrase, _score in keywords:\n",
    "        candidate = normalize_ticket_text(phrase).lower()\n",
    "        if len(candidate) < 3:\n",
    "            continue\n",
    "        if candidate in seen:\n",
    "            continue\n",
    "        seen.add(candidate)\n",
    "        tags.append(candidate)\n",
    "        if len(tags) >= top_k:\n",
    "            break\n",
    "\n",
    "    return tags\n",
    "\n",
    "print(\"YAKE tag extractor ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0cb387de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary module ready.\n"
     ]
    }
   ],
   "source": [
    "# Cell 23: Summary module (t5-small)\n",
    "from transformers import pipeline\n",
    "\n",
    "SUMMARIZER = None\n",
    "\n",
    "def get_summarizer():\n",
    "    global SUMMARIZER\n",
    "    if SUMMARIZER is None:\n",
    "        model_name = CONFIG[\"model_names\"][\"summary\"]\n",
    "        print(f\"Loading summarizer: {model_name}\")\n",
    "        print(\"If first run, Hugging Face will download summarizer files.\")\n",
    "        device = 0 if torch.cuda.is_available() else -1\n",
    "        SUMMARIZER = pipeline(\"summarization\", model=model_name, tokenizer=model_name, device=device)\n",
    "    return SUMMARIZER\n",
    "\n",
    "def summarize_text(text: str, min_len: int = 12, max_len: int = 60):\n",
    "    clean_text = normalize_ticket_text(text)\n",
    "    if not clean_text:\n",
    "        return \"\"\n",
    "\n",
    "    if len(clean_text.split()) < 25:\n",
    "        return clean_text\n",
    "\n",
    "    clean_text = clean_text[:3000]\n",
    "    summarizer = get_summarizer()\n",
    "    prefixed = f\"summarize: {clean_text}\"\n",
    "\n",
    "    output = summarizer(\n",
    "        prefixed,\n",
    "        max_length=max_len,\n",
    "        min_length=min_len,\n",
    "        do_sample=False,\n",
    "        truncation=True,\n",
    "    )\n",
    "    return normalize_ticket_text(output[0][\"summary_text\"])\n",
    "\n",
    "print(\"Summary module ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bb11acfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unified triage function ready: triage_ticket(ticket_text)\n"
     ]
    }
   ],
   "source": [
    "# Cell 24: Unified inference function -> JSON\n",
    "from typing import Dict\n",
    "\n",
    "CLASSIFIER_CACHE = {}\n",
    "\n",
    "def _get_label_maps(task_name: str):\n",
    "    return TASK_DATA[task_name][\"label2id\"], TASK_DATA[task_name][\"id2label\"]\n",
    "\n",
    "def _best_model_dir(task_name: str) -> Path:\n",
    "    return MODELS_DIR / f\"{task_name}_model\" / \"best\"\n",
    "\n",
    "def _get_transformer_runtime(task_name: str):\n",
    "    cache_key = f\"transformer::{task_name}\"\n",
    "    if cache_key in CLASSIFIER_CACHE:\n",
    "        return CLASSIFIER_CACHE[cache_key]\n",
    "\n",
    "    trainer_var = \"DEPT_BEST_TRAINER\" if task_name == \"department\" else \"URGENCY_BEST_TRAINER\"\n",
    "    tokenizer_var = \"DEPT_BEST_TOKENIZER\" if task_name == \"department\" else \"URGENCY_BEST_TOKENIZER\"\n",
    "    if trainer_var in globals() and tokenizer_var in globals() and globals()[trainer_var] is not None:\n",
    "        runtime = {\n",
    "            \"mode\": \"in_memory_transformer\",\n",
    "            \"model\": globals()[trainer_var].model,\n",
    "            \"tokenizer\": globals()[tokenizer_var],\n",
    "        }\n",
    "        CLASSIFIER_CACHE[cache_key] = runtime\n",
    "        return runtime\n",
    "\n",
    "    model_dir = _best_model_dir(task_name)\n",
    "    if model_dir.exists():\n",
    "        runtime = {\n",
    "            \"mode\": \"disk_transformer\",\n",
    "            \"model\": AutoModelForSequenceClassification.from_pretrained(str(model_dir)),\n",
    "            \"tokenizer\": AutoTokenizer.from_pretrained(str(model_dir), use_fast=True),\n",
    "        }\n",
    "        CLASSIFIER_CACHE[cache_key] = runtime\n",
    "        return runtime\n",
    "\n",
    "    if \"BASELINE_MODELS\" in globals() and task_name in BASELINE_MODELS:\n",
    "        runtime = {\n",
    "            \"mode\": \"baseline\",\n",
    "            \"model\": BASELINE_MODELS[task_name],\n",
    "            \"tokenizer\": None,\n",
    "        }\n",
    "        CLASSIFIER_CACHE[cache_key] = runtime\n",
    "        return runtime\n",
    "\n",
    "    raise RuntimeError(\n",
    "        f\"No model available for task '{task_name}'. Run training cells first \"\n",
    "        f\"or ensure baseline cell executed.\"\n",
    "    )\n",
    "\n",
    "def _predict_label(task_name: str, text: str):\n",
    "    _label2id, id2label = _get_label_maps(task_name)\n",
    "    runtime = _get_transformer_runtime(task_name)\n",
    "\n",
    "    if runtime[\"mode\"] == \"baseline\":\n",
    "        probs = runtime[\"model\"].predict_proba([text])[0]\n",
    "        pred_id = int(np.argmax(probs))\n",
    "        conf = float(probs[pred_id])\n",
    "    else:\n",
    "        model = runtime[\"model\"]\n",
    "        tokenizer = runtime[\"tokenizer\"]\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        enc = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=CONFIG[\"train\"][\"max_length\"],\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        enc = {k: v.to(device) for k, v in enc.items()}\n",
    "        with torch.no_grad():\n",
    "            logits = model(**enc).logits\n",
    "            probs = torch.softmax(logits, dim=-1).squeeze(0).detach().cpu().numpy()\n",
    "        pred_id = int(np.argmax(probs))\n",
    "        conf = float(probs[pred_id])\n",
    "\n",
    "    label = id2label[pred_id]\n",
    "    return {\n",
    "        \"label\": label,\n",
    "        \"confidence\": round(conf, 6),\n",
    "    }\n",
    "\n",
    "def triage_ticket(ticket_text: str, ticket_id: str = \"ad_hoc\", top_k_tags: int = 5, include_summary: bool = True) -> Dict:\n",
    "    clean_text = normalize_ticket_text(ticket_text)\n",
    "    if not clean_text:\n",
    "        raise ValueError(\"ticket_text is empty after preprocessing\")\n",
    "\n",
    "    department_pred = _predict_label(\"department\", clean_text)\n",
    "    urgency_pred = _predict_label(\"urgency\", clean_text)\n",
    "    tags = extract_tags(clean_text, top_k=top_k_tags)\n",
    "    summary = summarize_text(clean_text) if include_summary else \"\"\n",
    "\n",
    "    return {\n",
    "        \"ticket_id\": str(ticket_id),\n",
    "        \"department\": department_pred,\n",
    "        \"urgency\": urgency_pred,\n",
    "        \"tags\": tags,\n",
    "        \"summary\": summary,\n",
    "    }\n",
    "\n",
    "print(\"Unified triage function ready: triage_ticket(ticket_text)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a795f204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading summarizer: t5-small\n",
      "If first run, Hugging Face will download summarizer files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Your max_length is set to 60, but your input_length is only 38. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=19)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demo tickets processed: 8\n",
      "Demo summary mode: model\n",
      "\n",
      "--- Demo Output 1 ---\n",
      "{\n",
      "  \"ticket_id\": \"28906\",\n",
      "  \"department\": {\n",
      "    \"label\": \"Billing and Payments\",\n",
      "    \"confidence\": 0.921739\n",
      "  },\n",
      "  \"urgency\": {\n",
      "    \"label\": \"medium\",\n",
      "    \"confidence\": 0.431579\n",
      "  },\n",
      "  \"tags\": [\n",
      "    \"customer support team,i\",\n",
      "    \"support team,i hope\",\n",
      "    \"customer support\",\n",
      "    \"aws management service\",\n",
      "    \"support team,i\"\n",
      "  ],\n",
      "  \"summary\": \"the billing reflects charges that were not discussed or predicted in previous correspondence regarding expected usage rates and associated expenses . my team conducted an internal audit and found differences in the projections versus the billed amounts in the billing documentation provided by you on date.\"\n",
      "}\n",
      "\n",
      "--- Demo Output 2 ---\n",
      "{\n",
      "  \"ticket_id\": \"28840\",\n",
      "  \"department\": {\n",
      "    \"label\": \"Technical Support\",\n",
      "    \"confidence\": 0.343191\n",
      "  },\n",
      "  \"urgency\": {\n",
      "    \"label\": \"high\",\n",
      "    \"confidence\": 0.437415\n",
      "  },\n",
      "  \"tags\": [\n",
      "    \"customer support team,i\",\n",
      "    \"support team,i hope\",\n",
      "    \"customer support\",\n",
      "    \"team,i hope\",\n",
      "    \"hope this message\"\n",
      "  ],\n",
      "  \"summary\": \"our company relies heavily on the IT Consulting Service provided by your firm . the server disruption has persisted despite our in-house troubleshooting efforts .\"\n",
      "}\n",
      "\n",
      "--- Demo Output 3 ---\n",
      "{\n",
      "  \"ticket_id\": \"3787\",\n",
      "  \"department\": {\n",
      "    \"label\": \"Customer Service\",\n",
      "    \"confidence\": 0.267399\n",
      "  },\n",
      "  \"urgency\": {\n",
      "    \"label\": \"medium\",\n",
      "    \"confidence\": 0.41763\n",
      "  },\n",
      "  \"tags\": [\n",
      "    \"scalable saas project\",\n",
      "    \"saas project management\",\n",
      "    \"project management platform\",\n",
      "    \"customer support,i\",\n",
      "    \"support,i am reaching\"\n",
      "  ],\n",
      "  \"summary\": \"customer support would like to know the steps involved in setting up your SaaS project management platform .\"\n",
      "}\n",
      "Saved demo predictions: /Users/rawadyared/NLP-IT-Ticket_Triage/results/sample_predictions.jsonl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticket_id</th>\n",
       "      <th>department</th>\n",
       "      <th>dept_conf</th>\n",
       "      <th>urgency</th>\n",
       "      <th>urg_conf</th>\n",
       "      <th>tags</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28906</td>\n",
       "      <td>Billing and Payments</td>\n",
       "      <td>0.921739</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.431579</td>\n",
       "      <td>customer support team,i, support team,i hope, customer support, aws management service, support team,i</td>\n",
       "      <td>the billing reflects charges that were not discussed or predicted in previous correspondence regarding expected usage rates and associated expenses . my tea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28840</td>\n",
       "      <td>Technical Support</td>\n",
       "      <td>0.343191</td>\n",
       "      <td>high</td>\n",
       "      <td>0.437415</td>\n",
       "      <td>customer support team,i, support team,i hope, customer support, team,i hope, hope this message</td>\n",
       "      <td>our company relies heavily on the IT Consulting Service provided by your firm . the server disruption has persisted despite our in-house troubleshooting eff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3787</td>\n",
       "      <td>Customer Service</td>\n",
       "      <td>0.267399</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.417630</td>\n",
       "      <td>scalable saas project, saas project management, project management platform, customer support,i, support,i am reaching</td>\n",
       "      <td>customer support would like to know the steps involved in setting up your SaaS project management platform .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24432</td>\n",
       "      <td>Product Support</td>\n",
       "      <td>0.268937</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.422503</td>\n",
       "      <td>protocols are advised, advised, protocols</td>\n",
       "      <td>Which protocols are advised?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12272</td>\n",
       "      <td>Technical Support</td>\n",
       "      <td>0.274438</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.418879</td>\n",
       "      <td>customer support, project management saas, jenkins project management, management saas, support</td>\n",
       "      <td>customer support would appreciate a detailed guide or documentation set up . could include troubleshooting tips for common issues .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22197</td>\n",
       "      <td>Product Support</td>\n",
       "      <td>0.280785</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.427547</td>\n",
       "      <td>customer support, digital strategy tools, boost brand growth, brand growth integration, hubspot and clickup</td>\n",
       "      <td>the enhancements would allow the team to streamline workflows, track progress, and make data-driven decisions more efficiently . current tools are limited a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12696</td>\n",
       "      <td>Technical Support</td>\n",
       "      <td>0.405435</td>\n",
       "      <td>high</td>\n",
       "      <td>0.403304</td>\n",
       "      <td>customer support, customer, support, data has occurred, access</td>\n",
       "      <td>an incident involving unauthorized access to medical data has occurred . access issues persist .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6904</td>\n",
       "      <td>Technical Support</td>\n",
       "      <td>0.441482</td>\n",
       "      <td>high</td>\n",
       "      <td>0.443826</td>\n",
       "      <td>unauthorized access alert, medical data systems, software vulnerability, reset access credentials, unauthorized access</td>\n",
       "      <td>an unauthorized access alert has been detected in our medical data systems . we have applied a patch to the affected software, reset access credentials .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticket_id            department  dept_conf urgency  urg_conf  \\\n",
       "0     28906  Billing and Payments   0.921739  medium  0.431579   \n",
       "1     28840     Technical Support   0.343191    high  0.437415   \n",
       "2      3787      Customer Service   0.267399  medium  0.417630   \n",
       "3     24432       Product Support   0.268937  medium  0.422503   \n",
       "4     12272     Technical Support   0.274438  medium  0.418879   \n",
       "5     22197       Product Support   0.280785  medium  0.427547   \n",
       "6     12696     Technical Support   0.405435    high  0.403304   \n",
       "7      6904     Technical Support   0.441482    high  0.443826   \n",
       "\n",
       "                                                                                                                     tags  \\\n",
       "0                  customer support team,i, support team,i hope, customer support, aws management service, support team,i   \n",
       "1                          customer support team,i, support team,i hope, customer support, team,i hope, hope this message   \n",
       "2  scalable saas project, saas project management, project management platform, customer support,i, support,i am reaching   \n",
       "3                                                                               protocols are advised, advised, protocols   \n",
       "4                         customer support, project management saas, jenkins project management, management saas, support   \n",
       "5             customer support, digital strategy tools, boost brand growth, brand growth integration, hubspot and clickup   \n",
       "6                                                          customer support, customer, support, data has occurred, access   \n",
       "7  unauthorized access alert, medical data systems, software vulnerability, reset access credentials, unauthorized access   \n",
       "\n",
       "                                                                                                                                                           summary  \n",
       "0  the billing reflects charges that were not discussed or predicted in previous correspondence regarding expected usage rates and associated expenses . my tea...  \n",
       "1  our company relies heavily on the IT Consulting Service provided by your firm . the server disruption has persisted despite our in-house troubleshooting eff...  \n",
       "2                                                     customer support would like to know the steps involved in setting up your SaaS project management platform .  \n",
       "3                                                                                                                                     Which protocols are advised?  \n",
       "4                              customer support would appreciate a detailed guide or documentation set up . could include troubleshooting tips for common issues .  \n",
       "5  the enhancements would allow the team to streamline workflows, track progress, and make data-driven decisions more efficiently . current tools are limited a...  \n",
       "6                                                                 an incident involving unauthorized access to medical data has occurred . access issues persist .  \n",
       "7        an unauthorized access alert has been detected in our medical data systems . we have applied a patch to the affected software, reset access credentials .  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 25: Run demo on 5-10 sample tickets + save sample predictions\n",
    "import json\n",
    "\n",
    "DEMO_SIZE = 8\n",
    "DEMO_INCLUDE_SUMMARY = True\n",
    "id_col = CONFIG[\"id_column\"]\n",
    "\n",
    "working = df_raw[[id_col, \"ticket_text\"]].copy()\n",
    "working[\"word_count\"] = working[\"ticket_text\"].str.split().str.len()\n",
    "\n",
    "long_idx = working.nlargest(2, \"word_count\").index.tolist()\n",
    "question_mask = working[\"ticket_text\"].str.contains(r\"\\?\", regex=True, na=False)\n",
    "question_pool = working[question_mask]\n",
    "question_idx = question_pool.sample(n=min(2, len(question_pool)), random_state=CONFIG[\"seed\"]).index.tolist() if len(question_pool) > 0 else []\n",
    "\n",
    "remaining = max(0, DEMO_SIZE - len(long_idx) - len(question_idx))\n",
    "random_pool = working.drop(index=set(long_idx + question_idx), errors=\"ignore\")\n",
    "rand_idx = random_pool.sample(n=min(remaining, len(random_pool)), random_state=CONFIG[\"seed\"]).index.tolist()\n",
    "\n",
    "demo_idx = list(dict.fromkeys(long_idx + question_idx + rand_idx))\n",
    "demo_df = working.loc[demo_idx].reset_index(drop=True)\n",
    "\n",
    "demo_outputs = []\n",
    "demo_summary_mode = \"model\"\n",
    "for row in demo_df.itertuples(index=False):\n",
    "    try:\n",
    "        result = triage_ticket(\n",
    "            ticket_text=row.ticket_text,\n",
    "            ticket_id=row[0],\n",
    "            top_k_tags=5,\n",
    "            include_summary=DEMO_INCLUDE_SUMMARY,\n",
    "        )\n",
    "    except Exception:\n",
    "        demo_summary_mode = \"fallback_no_summary\"\n",
    "        result = triage_ticket(\n",
    "            ticket_text=row.ticket_text,\n",
    "            ticket_id=row[0],\n",
    "            top_k_tags=5,\n",
    "            include_summary=False,\n",
    "        )\n",
    "    demo_outputs.append(result)\n",
    "\n",
    "print(f\"Demo tickets processed: {len(demo_outputs)}\")\n",
    "print(f\"Demo summary mode: {demo_summary_mode}\")\n",
    "for i, payload in enumerate(demo_outputs[:3], start=1):\n",
    "    print(f\"\\n--- Demo Output {i} ---\")\n",
    "    print(json.dumps(payload, ensure_ascii=True, indent=2))\n",
    "\n",
    "sample_jsonl_path = RESULTS_DIR / \"sample_predictions.jsonl\"\n",
    "with open(sample_jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for payload in demo_outputs:\n",
    "        f.write(json.dumps(payload, ensure_ascii=True) + \"\\n\")\n",
    "print(f\"Saved demo predictions: {sample_jsonl_path}\")\n",
    "\n",
    "demo_view = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"ticket_id\": p[\"ticket_id\"],\n",
    "            \"department\": p[\"department\"][\"label\"],\n",
    "            \"dept_conf\": p[\"department\"][\"confidence\"],\n",
    "            \"urgency\": p[\"urgency\"][\"label\"],\n",
    "            \"urg_conf\": p[\"urgency\"][\"confidence\"],\n",
    "            \"tags\": \", \".join(p[\"tags\"]),\n",
    "            \"summary\": p[\"summary\"],\n",
    "        }\n",
    "        for p in demo_outputs\n",
    "    ]\n",
    ")\n",
    "display(demo_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1e9a72b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Your max_length is set to 60, but your input_length is only 36. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=18)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Your max_length is set to 60, but your input_length is only 58. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=29)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Your max_length is set to 60, but your input_length is only 59. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=29)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Your max_length is set to 60, but your input_length is only 49. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=24)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Your max_length is set to 60, but your input_length is only 53. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=26)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Your max_length is set to 60, but your input_length is only 52. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=26)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Your max_length is set to 60, but your input_length is only 52. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=26)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Your max_length is set to 60, but your input_length is only 47. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=23)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Your max_length is set to 60, but your input_length is only 56. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=28)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Your max_length is set to 60, but your input_length is only 38. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=19)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Your max_length is set to 60, but your input_length is only 56. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=28)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Your max_length is set to 60, but your input_length is only 48. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=24)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Your max_length is set to 60, but your input_length is only 50. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=25)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Your max_length is set to 60, but your input_length is only 39. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=19)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Your max_length is set to 60, but your input_length is only 38. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=19)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Your max_length is set to 60, but your input_length is only 36. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=18)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Your max_length is set to 60, but your input_length is only 58. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=29)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business Value Summary\n",
      "- Avg ticket words (full dataset): 55.79\n",
      "- Manual triage time/ticket: 46.74s\n",
      "- Model triage time/ticket: 5.00s\n",
      "- Time saved per ticket: 41.74s\n",
      "- Monthly hours saved: 34.78h\n",
      "- Monthly savings (USD): $973.83\n",
      "- Annual savings (USD): $11,686.00\n",
      "- Avg input words (summary sample): 51.76\n",
      "- Avg summary words (summary sample): 22.14\n",
      "- Summary/Input ratio: 0.428\n",
      "Saved business report: /Users/rawadyared/NLP-IT-Ticket_Triage/results/business_value_analysis.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>input_words</th>\n",
       "      <th>summary_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26115</td>\n",
       "      <td>76</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10543</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17539</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18419</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7318</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>17021</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>11384</td>\n",
       "      <td>27</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>26955</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10494</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>23959</td>\n",
       "      <td>43</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  input_words  summary_words\n",
       "0       26115           76             23\n",
       "1       10543           15             15\n",
       "2       17539           24             24\n",
       "3       18419           11             11\n",
       "4        7318           10             10\n",
       "5       17021            6              6\n",
       "6       11384           27             15\n",
       "7       26955           19             19\n",
       "8       10494            7              7\n",
       "9       23959           43             28"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 26: Business value analysis + summary-length comparison\n",
    "import time\n",
    "\n",
    "ASSUMPTIONS = {\n",
    "    \"reading_speed_wpm\": 200,\n",
    "    \"routing_buffer_seconds\": 30,\n",
    "    \"model_seconds_per_ticket\": 5,\n",
    "    \"hourly_wage_usd\": 28.0,\n",
    "    \"monthly_ticket_volume\": int(len(df_raw)),\n",
    "    \"summary_eval_sample_size\": min(100, int(len(df_raw))),\n",
    "}\n",
    "\n",
    "id_col = CONFIG[\"id_column\"]\n",
    "\n",
    "word_counts = df_raw[\"ticket_text\"].str.split().str.len()\n",
    "avg_ticket_words = float(word_counts.mean())\n",
    "\n",
    "manual_read_seconds = (avg_ticket_words / ASSUMPTIONS[\"reading_speed_wpm\"]) * 60.0\n",
    "manual_total_seconds = manual_read_seconds + ASSUMPTIONS[\"routing_buffer_seconds\"]\n",
    "model_total_seconds = float(ASSUMPTIONS[\"model_seconds_per_ticket\"])\n",
    "time_saved_seconds = manual_total_seconds - model_total_seconds\n",
    "\n",
    "hourly_wage = ASSUMPTIONS[\"hourly_wage_usd\"]\n",
    "monthly_volume = ASSUMPTIONS[\"monthly_ticket_volume\"]\n",
    "\n",
    "monthly_hours_saved = (time_saved_seconds * monthly_volume) / 3600.0\n",
    "monthly_savings_usd = monthly_hours_saved * hourly_wage\n",
    "annual_savings_usd = monthly_savings_usd * 12.0\n",
    "\n",
    "# Summary-length analysis on a fixed sample for runtime practicality.\n",
    "sample_n = ASSUMPTIONS[\"summary_eval_sample_size\"]\n",
    "summary_sample = df_raw[[id_col, \"ticket_text\"]].sample(n=sample_n, random_state=CONFIG[\"seed\"]).reset_index(drop=True)\n",
    "\n",
    "summary_outputs = []\n",
    "start_ts = time.time()\n",
    "summary_mode = \"model\"\n",
    "for txt in summary_sample[\"ticket_text\"]:\n",
    "    try:\n",
    "        summary_outputs.append(summarize_text(txt))\n",
    "    except Exception:\n",
    "        summary_mode = \"fallback\"\n",
    "        fallback = \" \".join(normalize_ticket_text(txt).split()[:40])\n",
    "        summary_outputs.append(fallback)\n",
    "elapsed_summary_sec = time.time() - start_ts\n",
    "\n",
    "summary_sample[\"summary_text\"] = summary_outputs\n",
    "summary_sample[\"input_words\"] = summary_sample[\"ticket_text\"].str.split().str.len()\n",
    "summary_sample[\"summary_words\"] = summary_sample[\"summary_text\"].str.split().str.len()\n",
    "\n",
    "avg_input_words_sample = float(summary_sample[\"input_words\"].mean())\n",
    "avg_summary_words_sample = float(summary_sample[\"summary_words\"].mean())\n",
    "compression_ratio = avg_summary_words_sample / max(avg_input_words_sample, 1e-9)\n",
    "\n",
    "business_value_report = {\n",
    "    \"assumptions\": ASSUMPTIONS,\n",
    "    \"dataset_stats\": {\n",
    "        \"num_tickets\": int(len(df_raw)),\n",
    "        \"avg_ticket_words\": round(avg_ticket_words, 4),\n",
    "    },\n",
    "    \"timing_seconds\": {\n",
    "        \"manual_read_seconds\": round(manual_read_seconds, 4),\n",
    "        \"manual_total_seconds\": round(manual_total_seconds, 4),\n",
    "        \"model_total_seconds\": round(model_total_seconds, 4),\n",
    "        \"time_saved_seconds_per_ticket\": round(time_saved_seconds, 4),\n",
    "    },\n",
    "    \"savings_usd\": {\n",
    "        \"monthly_hours_saved\": round(monthly_hours_saved, 4),\n",
    "        \"monthly_savings\": round(monthly_savings_usd, 2),\n",
    "        \"annual_savings\": round(annual_savings_usd, 2),\n",
    "    },\n",
    "    \"summary_length_analysis\": {\n",
    "        \"summary_mode\": summary_mode,\n",
    "        \"sample_size\": int(sample_n),\n",
    "        \"avg_input_words_sample\": round(avg_input_words_sample, 4),\n",
    "        \"avg_summary_words_sample\": round(avg_summary_words_sample, 4),\n",
    "        \"compression_ratio_summary_over_input\": round(compression_ratio, 4),\n",
    "        \"summary_eval_elapsed_seconds\": round(elapsed_summary_sec, 4),\n",
    "    },\n",
    "}\n",
    "\n",
    "business_path = RESULTS_DIR / \"business_value_analysis.json\"\n",
    "with open(business_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(business_value_report, f, ensure_ascii=True, indent=2)\n",
    "\n",
    "print(\"Business Value Summary\")\n",
    "print(f\"- Avg ticket words (full dataset): {avg_ticket_words:.2f}\")\n",
    "print(f\"- Manual triage time/ticket: {manual_total_seconds:.2f}s\")\n",
    "print(f\"- Model triage time/ticket: {model_total_seconds:.2f}s\")\n",
    "print(f\"- Time saved per ticket: {time_saved_seconds:.2f}s\")\n",
    "print(f\"- Monthly hours saved: {monthly_hours_saved:.2f}h\")\n",
    "print(f\"- Monthly savings (USD): ${monthly_savings_usd:,.2f}\")\n",
    "print(f\"- Annual savings (USD): ${annual_savings_usd:,.2f}\")\n",
    "print(f\"- Avg input words (summary sample): {avg_input_words_sample:.2f}\")\n",
    "print(f\"- Avg summary words (summary sample): {avg_summary_words_sample:.2f}\")\n",
    "print(f\"- Summary/Input ratio: {compression_ratio:.3f}\")\n",
    "print(f\"Saved business report: {business_path}\")\n",
    "\n",
    "display(summary_sample[[id_col, \"input_words\", \"summary_words\"]].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a2c9f9-0626-46e9-870a-3e32d318a236",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61154be4-3c27-4fa3-bf95-cc9ae978a3d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
