{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f8ee623",
   "metadata": {},
   "source": [
    "# IT Ticket Triage - Training & Evaluation (Dept + Urgency + Tags/Summary)\n",
    "\n",
    "This notebook trains and evaluates the NLP triage pipeline in phases.\n",
    "\n",
    "- Trains: department routing classifier and urgency/priority classifier\n",
    "- Prototypes: tag extraction and summary generation\n",
    "- Produces: metrics, saved model artifacts, and inference-ready mappings\n",
    "\n",
    "Target inference JSON schema:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"ticket_id\": \"...\",\n",
    "  \"department\": {\"label\": \"...\", \"confidence\": 0.92},\n",
    "  \"urgency\": {\"label\": \"...\", \"confidence\": 0.81},\n",
    "  \"tags\": [\"vpn\", \"login\", \"timeout\"],\n",
    "  \"summary\": \"User cannot connect to VPN after password reset.\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8c83fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.4\n",
      "Torch: 2.10.0\n",
      "CUDA available: False\n",
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Environment sanity + GPU check + deterministic seeds\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "print(f\"Python: {sys.version.split()[0]}\")\n",
    "print(f\"Torch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(SEED)\n",
    "print(f\"Seed set to {SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0ffb5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Install dependencies (single requirements cell)\n",
    "%pip -q install -U \"numpy==1.26.4\" \"pandas==2.2.2\" \"scikit-learn>=1.4,<2\" \"transformers<5\" \"datasets<3\" \"evaluate<1\" \"accelerate<1\" \"yake<1\" \"sentencepiece<1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e57db188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Imports\n",
    "import json\n",
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import datasets\n",
    "import evaluate\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_colwidth\", 160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "501868b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolved BASE_DIR: /Users/rawadyared/NLP-IT-Ticket_Triage\n",
      "{\n",
      "  \"dataset_path\": \"/Users/rawadyared/NLP-IT-Ticket_Triage/data/raw/IT Support Ticket Data.csv\",\n",
      "  \"id_column\": \"Unnamed: 0\",\n",
      "  \"text_columns\": [\n",
      "    \"Body\"\n",
      "  ],\n",
      "  \"label_columns\": {\n",
      "    \"department\": \"Department\",\n",
      "    \"urgency\": \"Priority\"\n",
      "  },\n",
      "  \"model_names\": {\n",
      "    \"department\": \"distilroberta-base\",\n",
      "    \"urgency\": \"distilroberta-base\",\n",
      "    \"summary\": \"t5-small\"\n",
      "  },\n",
      "  \"candidate_models\": {\n",
      "    \"department\": [\n",
      "      \"distilroberta-base\",\n",
      "      \"bert-base-uncased\"\n",
      "    ],\n",
      "    \"urgency\": [\n",
      "      \"distilroberta-base\",\n",
      "      \"bert-base-uncased\"\n",
      "    ]\n",
      "  },\n",
      "  \"train\": {\n",
      "    \"max_length\": 256,\n",
      "    \"batch_size\": 16,\n",
      "    \"learning_rate\": 2e-05,\n",
      "    \"epochs\": 3,\n",
      "    \"weight_decay\": 0.01,\n",
      "    \"warmup_ratio\": 0.1\n",
      "  },\n",
      "  \"split\": {\n",
      "    \"train_size\": 0.8,\n",
      "    \"val_size\": 0.1,\n",
      "    \"test_size\": 0.1\n",
      "  },\n",
      "  \"preprocess\": {\n",
      "    \"lowercase\": false,\n",
      "    \"remove_boilerplate\": true\n",
      "  },\n",
      "  \"paths\": {\n",
      "    \"results_dir\": \"/Users/rawadyared/NLP-IT-Ticket_Triage/results\",\n",
      "    \"models_dir\": \"/Users/rawadyared/NLP-IT-Ticket_Triage/models\",\n",
      "    \"mappings_dir\": \"/Users/rawadyared/NLP-IT-Ticket_Triage/results/mappings\"\n",
      "  },\n",
      "  \"experiment\": {\n",
      "    \"candidate_epochs\": 1,\n",
      "    \"run_candidate_search\": true\n",
      "  },\n",
      "  \"seed\": 42\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Repo paths + config (single source of truth)\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "if \"SEED\" not in globals():\n",
    "    SEED = 42\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "def detect_base_dir() -> Path:\n",
    "    if IN_COLAB:\n",
    "        preferred = Path(\"/content/NLP-IT-Ticket_Triage\")\n",
    "        if preferred.exists():\n",
    "            return preferred\n",
    "        return Path(\"/content\")\n",
    "\n",
    "    cwd = Path.cwd().resolve()\n",
    "    candidate_name = Path(\"data\") / \"raw\" / \"IT Support Ticket Data.csv\"\n",
    "\n",
    "    search_roots = [cwd, *cwd.parents]\n",
    "    for root in search_roots:\n",
    "        if (root / candidate_name).exists():\n",
    "            return root\n",
    "    return cwd\n",
    "\n",
    "BASE_DIR = detect_base_dir()\n",
    "print(f\"Resolved BASE_DIR: {BASE_DIR}\")\n",
    "\n",
    "DATASET_CANDIDATES = [\n",
    "    BASE_DIR / \"data\" / \"raw\" / \"IT Support Ticket Data.csv\",\n",
    "    Path(\"/content\") / \"IT Support Ticket Data.csv\",\n",
    "]\n",
    "\n",
    "def resolve_dataset_path(candidates):\n",
    "    for path in candidates:\n",
    "        if path.exists():\n",
    "            return path\n",
    "    candidate_list = \"\\n\".join([str(p) for p in candidates])\n",
    "    raise FileNotFoundError(f\"Dataset not found. Checked:\\n{candidate_list}\")\n",
    "\n",
    "OUTPUT_ROOT = Path(\"/content\") if IN_COLAB else BASE_DIR\n",
    "RESULTS_DIR = OUTPUT_ROOT / \"results\"\n",
    "MODELS_DIR = OUTPUT_ROOT / \"models\"\n",
    "MAPPINGS_DIR = RESULTS_DIR / \"mappings\"\n",
    "\n",
    "for out_dir in [RESULTS_DIR, MODELS_DIR, MAPPINGS_DIR]:\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CONFIG = {\n",
    "    \"dataset_path\": str(resolve_dataset_path(DATASET_CANDIDATES)),\n",
    "    \"id_column\": \"Unnamed: 0\",\n",
    "    \"text_columns\": [\"Body\"],\n",
    "    \"label_columns\": {\n",
    "        \"department\": \"Department\",\n",
    "        \"urgency\": \"Priority\",\n",
    "    },\n",
    "    \"model_names\": {\n",
    "        \"department\": \"distilroberta-base\",\n",
    "        \"urgency\": \"distilroberta-base\",\n",
    "        \"summary\": \"t5-small\",\n",
    "    },\n",
    "    \"candidate_models\": {\n",
    "        \"department\": [\"distilroberta-base\", \"bert-base-uncased\"],\n",
    "        \"urgency\": [\"distilroberta-base\", \"bert-base-uncased\"],\n",
    "    },\n",
    "    \"train\": {\n",
    "        \"max_length\": 256,\n",
    "        \"batch_size\": 16,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"epochs\": 3,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"warmup_ratio\": 0.1,\n",
    "    },\n",
    "    \"split\": {\n",
    "        \"train_size\": 0.8,\n",
    "        \"val_size\": 0.1,\n",
    "        \"test_size\": 0.1,\n",
    "    },\n",
    "    \"preprocess\": {\n",
    "        \"lowercase\": False,\n",
    "        \"remove_boilerplate\": True\n",
    "    },\n",
    "    \"paths\": {\n",
    "        \"results_dir\": str(RESULTS_DIR),\n",
    "        \"models_dir\": str(MODELS_DIR),\n",
    "        \"mappings_dir\": str(MAPPINGS_DIR),\n",
    "    },\n",
    "    \"experiment\": {\n",
    "        \"candidate_epochs\": 1,\n",
    "        \"run_candidate_search\": True\n",
    "    },\n",
    "    \"seed\": SEED,\n",
    "}\n",
    "\n",
    "print(json.dumps(CONFIG, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46a57edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (29651, 5)\n",
      "Columns: ['Unnamed: 0', 'Body', 'Department', 'Priority', 'Tags']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Body</th>\n",
       "      <th>Department</th>\n",
       "      <th>Priority</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Dear Customer Support Team,I am writing to report a significant problem with the centralized account management portal, which currently appears to be offlin...</td>\n",
       "      <td>Technical Support</td>\n",
       "      <td>high</td>\n",
       "      <td>['Account', 'Disruption', 'Outage', 'IT', 'Tech Support']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Dear Customer Support Team,I hope this message reaches you well. I am reaching out to request detailed information about the capabilities of your smart home...</td>\n",
       "      <td>Returns and Exchanges</td>\n",
       "      <td>medium</td>\n",
       "      <td>['Product', 'Feature', 'Tech Support']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Dear Customer Support Team,I hope this message finds you well. I am reaching out to request clarification about the billing and payment procedures linked to...</td>\n",
       "      <td>Billing and Payments</td>\n",
       "      <td>low</td>\n",
       "      <td>['Billing', 'Payment', 'Account', 'Documentation', 'Feedback']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  \\\n",
       "0           0   \n",
       "1           1   \n",
       "2           2   \n",
       "\n",
       "                                                                                                                                                              Body  \\\n",
       "0  Dear Customer Support Team,I am writing to report a significant problem with the centralized account management portal, which currently appears to be offlin...   \n",
       "1  Dear Customer Support Team,I hope this message reaches you well. I am reaching out to request detailed information about the capabilities of your smart home...   \n",
       "2  Dear Customer Support Team,I hope this message finds you well. I am reaching out to request clarification about the billing and payment procedures linked to...   \n",
       "\n",
       "              Department Priority  \\\n",
       "0      Technical Support     high   \n",
       "1  Returns and Exchanges   medium   \n",
       "2   Billing and Payments      low   \n",
       "\n",
       "                                                             Tags  \n",
       "0       ['Account', 'Disruption', 'Outage', 'IT', 'Tech Support']  \n",
       "1                          ['Product', 'Feature', 'Tech Support']  \n",
       "2  ['Billing', 'Payment', 'Account', 'Documentation', 'Feedback']  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 6: Load dataset\n",
    "df_raw = pd.read_csv(CONFIG[\"dataset_path\"])\n",
    "print(f\"Dataset shape: {df_raw.shape}\")\n",
    "print(f\"Columns: {list(df_raw.columns)}\")\n",
    "display(df_raw.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f16ff539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using text columns: ['Body']\n",
      "Rows after text cleaning: 23318\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticket_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Customer Support,We are encountering a disruption in VPN-router connectivity that is impacting several devices, notably essential remote telemedicine system...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Currently facing sporadic connectivity difficulties with the cloud-native SaaS system. The suspected reason appears to be linked to orchestration resource d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Customer Service, I am reaching out to report an issue with the billing payment process on my account. Recently, there have been inconsistencies in the bill...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                        ticket_text\n",
       "7   Customer Support,We are encountering a disruption in VPN-router connectivity that is impacting several devices, notably essential remote telemedicine system...\n",
       "23  Currently facing sporadic connectivity difficulties with the cloud-native SaaS system. The suspected reason appears to be linked to orchestration resource d...\n",
       "24  Customer Service, I am reaching out to report an issue with the billing payment process on my account. Recently, there have been inconsistencies in the bill..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 7: Build ticket_text\n",
    "configured_cols = CONFIG[\"text_columns\"]\n",
    "available_text_cols = [col for col in configured_cols if col in df_raw.columns]\n",
    "\n",
    "if not available_text_cols:\n",
    "    fallback_cols = [col for col in [\"short_description\", \"description\", \"Body\"] if col in df_raw.columns]\n",
    "    if not fallback_cols:\n",
    "        raise ValueError(\"No usable text columns found. Update CONFIG['text_columns'].\")\n",
    "    available_text_cols = fallback_cols\n",
    "\n",
    "print(f\"Using text columns: {available_text_cols}\")\n",
    "\n",
    "BOILERPLATE_PATTERNS = [\n",
    "    r\"(?i)^\\s*(dear|hi|hello)\\b[^\\n]*\",\n",
    "    r\"(?i)\\b(best regards|regards|thanks|thank you),?\\s*[\\w\\s\\.-]*$\",\n",
    "    r\"(?i)this email and any attachments are confidential.*$\",\n",
    "]\n",
    "\n",
    "def normalize_ticket_text(text: str) -> str:\n",
    "    text = \"\" if pd.isna(text) else str(text)\n",
    "    text = re.sub(r\"<[^>]+>\", \" \", text)\n",
    "\n",
    "    if CONFIG.get(\"preprocess\", {}).get(\"remove_boilerplate\", True):\n",
    "        for pattern in BOILERPLATE_PATTERNS:\n",
    "            text = re.sub(pattern, \" \", text, flags=re.MULTILINE | re.DOTALL)\n",
    "\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    if CONFIG.get(\"preprocess\", {}).get(\"lowercase\", False):\n",
    "        text = text.lower()\n",
    "    return text\n",
    "\n",
    "df_raw[\"ticket_text\"] = (\n",
    "    df_raw[available_text_cols]\n",
    "    .fillna(\"\")\n",
    "    .astype(str)\n",
    "    .agg(\" \".join, axis=1)\n",
    "    .map(normalize_ticket_text)\n",
    ")\n",
    "\n",
    "df_raw = df_raw[df_raw[\"ticket_text\"].str.len() > 0].copy()\n",
    "print(f\"Rows after text cleaning: {len(df_raw)}\")\n",
    "display(df_raw[[\"ticket_text\"]].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c19e73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[department] rows after label cleaning: 23318\n",
      "[department] num classes: 10\n",
      "[department] mapping saved: /Users/rawadyared/NLP-IT-Ticket_Triage/results/mappings/department_label_mapping.json\n",
      "[urgency] rows after label cleaning: 23318\n",
      "[urgency] num classes: 3\n",
      "[urgency] mapping saved: /Users/rawadyared/NLP-IT-Ticket_Triage/results/mappings/urgency_label_mapping.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Label cleaning + encoding\n",
    "def clean_and_encode_labels(df: pd.DataFrame, label_col: str, task_name: str, mappings_dir: Path):\n",
    "    if label_col not in df.columns:\n",
    "        raise KeyError(f\"Label column '{label_col}' not found for task '{task_name}'.\")\n",
    "\n",
    "    task_df = df.copy()\n",
    "    task_df = task_df[task_df[label_col].notna()].copy()\n",
    "    task_df[label_col] = task_df[label_col].astype(str).str.strip()\n",
    "    task_df = task_df[task_df[label_col] != \"\"].copy()\n",
    "\n",
    "    label_values = sorted(task_df[label_col].unique().tolist())\n",
    "    label2id = {label: idx for idx, label in enumerate(label_values)}\n",
    "    id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "    task_df[\"label_text\"] = task_df[label_col]\n",
    "    task_df[\"label\"] = task_df[\"label_text\"].map(label2id).astype(int)\n",
    "\n",
    "    mapping_payload = {\n",
    "        \"task\": task_name,\n",
    "        \"label_column\": label_col,\n",
    "        \"label2id\": label2id,\n",
    "        \"id2label\": {str(k): v for k, v in id2label.items()},\n",
    "    }\n",
    "    mapping_path = mappings_dir / f\"{task_name}_label_mapping.json\"\n",
    "    with open(mapping_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(mapping_payload, f, ensure_ascii=True, indent=2)\n",
    "\n",
    "    print(f\"[{task_name}] rows after label cleaning: {len(task_df)}\")\n",
    "    print(f\"[{task_name}] num classes: {len(label2id)}\")\n",
    "    print(f\"[{task_name}] mapping saved: {mapping_path}\")\n",
    "    return task_df, label2id, id2label\n",
    "\n",
    "dept_df, dept_label2id, dept_id2label = clean_and_encode_labels(\n",
    "    df=df_raw,\n",
    "    label_col=CONFIG[\"label_columns\"][\"department\"],\n",
    "    task_name=\"department\",\n",
    "    mappings_dir=MAPPINGS_DIR,\n",
    ")\n",
    "\n",
    "urgency_df, urgency_label2id, urgency_id2label = clean_and_encode_labels(\n",
    "    df=df_raw,\n",
    "    label_col=CONFIG[\"label_columns\"][\"urgency\"],\n",
    "    task_name=\"urgency\",\n",
    "    mappings_dir=MAPPINGS_DIR,\n",
    ")\n",
    "\n",
    "TASK_DATA = {\n",
    "    \"department\": {\n",
    "        \"df\": dept_df,\n",
    "        \"label2id\": dept_label2id,\n",
    "        \"id2label\": dept_id2label,\n",
    "    },\n",
    "    \"urgency\": {\n",
    "        \"df\": urgency_df,\n",
    "        \"label2id\": urgency_label2id,\n",
    "        \"id2label\": urgency_id2label,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "275b9022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[department] train: n=18654\n",
      "label_text\n",
      "Technical Support                  28.94\n",
      "Product Support                    18.91\n",
      "Customer Service                   14.92\n",
      "IT Support                         12.03\n",
      "Billing and Payments                9.84\n",
      "Returns and Exchanges               5.10\n",
      "Service Outages and Maintenance     3.93\n",
      "Sales and Pre-Sales                 2.91\n",
      "Human Resources                     2.09\n",
      "General Inquiry                     1.33\n",
      "\n",
      "[department] val: n=2332\n",
      "label_text\n",
      "Technical Support                  28.90\n",
      "Product Support                    18.91\n",
      "Customer Service                   14.92\n",
      "IT Support                         12.01\n",
      "Billing and Payments                9.86\n",
      "Returns and Exchanges               5.10\n",
      "Service Outages and Maintenance     3.95\n",
      "Sales and Pre-Sales                 2.92\n",
      "Human Resources                     2.10\n",
      "General Inquiry                     1.33\n",
      "\n",
      "[department] test: n=2332\n",
      "label_text\n",
      "Technical Support                  28.95\n",
      "Product Support                    18.91\n",
      "Customer Service                   14.92\n",
      "IT Support                         12.05\n",
      "Billing and Payments                9.82\n",
      "Returns and Exchanges               5.10\n",
      "Service Outages and Maintenance     3.95\n",
      "Sales and Pre-Sales                 2.92\n",
      "Human Resources                     2.06\n",
      "General Inquiry                     1.33\n",
      "\n",
      "[urgency] train: n=18654\n",
      "label_text\n",
      "medium    40.58\n",
      "high      38.97\n",
      "low       20.45\n",
      "\n",
      "[urgency] val: n=2332\n",
      "label_text\n",
      "medium    40.57\n",
      "high      38.98\n",
      "low       20.45\n",
      "\n",
      "[urgency] test: n=2332\n",
      "label_text\n",
      "medium    40.57\n",
      "high      38.98\n",
      "low       20.45\n",
      "\n",
      "Split generation complete for tasks: ['department', 'urgency']\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Train/Val/Test split (stratified) + leakage checks\n",
    "def stratified_split_with_leakage_guard(\n",
    "    task_df: pd.DataFrame,\n",
    "    id_col: str,\n",
    "    seed: int,\n",
    "    train_size: float,\n",
    "    val_size: float,\n",
    "    test_size: float,\n",
    "):\n",
    "    if not np.isclose(train_size + val_size + test_size, 1.0):\n",
    "        raise ValueError(\"Split sizes must sum to 1.0\")\n",
    "\n",
    "    working_df = task_df.copy()\n",
    "    if id_col not in working_df.columns:\n",
    "        working_df[id_col] = np.arange(len(working_df))\n",
    "\n",
    "    id_label = working_df[[id_col, \"label\"]].drop_duplicates()\n",
    "    label_per_id = id_label.groupby(id_col)[\"label\"].nunique()\n",
    "    if (label_per_id > 1).any():\n",
    "        raise ValueError(\"A ticket ID maps to multiple labels; cannot guarantee leakage-free split.\")\n",
    "\n",
    "    id_frame = id_label.drop_duplicates(subset=[id_col]).copy()\n",
    "    y = id_frame[\"label\"]\n",
    "    stratify_1 = y if y.value_counts().min() >= 2 else None\n",
    "\n",
    "    train_ids, temp_ids = train_test_split(\n",
    "        id_frame[id_col],\n",
    "        test_size=(1.0 - train_size),\n",
    "        random_state=seed,\n",
    "        stratify=stratify_1,\n",
    "    )\n",
    "\n",
    "    temp_frame = id_frame[id_frame[id_col].isin(temp_ids)].copy()\n",
    "    y_temp = temp_frame[\"label\"]\n",
    "    stratify_2 = y_temp if y_temp.value_counts().min() >= 2 else None\n",
    "    rel_test_size = test_size / (val_size + test_size)\n",
    "\n",
    "    val_ids, test_ids = train_test_split(\n",
    "        temp_frame[id_col],\n",
    "        test_size=rel_test_size,\n",
    "        random_state=seed,\n",
    "        stratify=stratify_2,\n",
    "    )\n",
    "\n",
    "    splits = {\n",
    "        \"train\": working_df[working_df[id_col].isin(set(train_ids))].reset_index(drop=True),\n",
    "        \"val\": working_df[working_df[id_col].isin(set(val_ids))].reset_index(drop=True),\n",
    "        \"test\": working_df[working_df[id_col].isin(set(test_ids))].reset_index(drop=True),\n",
    "    }\n",
    "\n",
    "    train_set = set(splits[\"train\"][id_col].tolist())\n",
    "    val_set = set(splits[\"val\"][id_col].tolist())\n",
    "    test_set = set(splits[\"test\"][id_col].tolist())\n",
    "    assert train_set.isdisjoint(val_set)\n",
    "    assert train_set.isdisjoint(test_set)\n",
    "    assert val_set.isdisjoint(test_set)\n",
    "\n",
    "    return splits\n",
    "\n",
    "def print_split_distribution(task_name: str, splits: dict):\n",
    "    for split_name, split_df in splits.items():\n",
    "        print(f\"\\n[{task_name}] {split_name}: n={len(split_df)}\")\n",
    "        dist = split_df[\"label_text\"].value_counts(normalize=True).mul(100).round(2)\n",
    "        print(dist.to_string())\n",
    "\n",
    "SPLITS = {}\n",
    "for task_name, task_info in TASK_DATA.items():\n",
    "    splits = stratified_split_with_leakage_guard(\n",
    "        task_df=task_info[\"df\"],\n",
    "        id_col=CONFIG[\"id_column\"],\n",
    "        seed=CONFIG[\"seed\"],\n",
    "        train_size=CONFIG[\"split\"][\"train_size\"],\n",
    "        val_size=CONFIG[\"split\"][\"val_size\"],\n",
    "        test_size=CONFIG[\"split\"][\"test_size\"],\n",
    "    )\n",
    "    SPLITS[task_name] = splits\n",
    "    print_split_distribution(task_name, splits)\n",
    "\n",
    "print(\"\\nSplit generation complete for tasks:\", list(SPLITS.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8213468f",
   "metadata": {},
   "source": [
    "## Baseline Plan (Justification)\n",
    "\n",
    "Before transformer fine-tuning, we establish classical NLP baselines using **TF-IDF + Logistic Regression**.\n",
    "\n",
    "- Provides a transparent performance floor for both targets.\n",
    "- Validates that improvements from transformers are meaningful.\n",
    "- Keeps evaluation consistent via accuracy, macro F1, and per-class metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4e08a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[department][val] accuracy=0.5828 macro_f1=0.6075 weighted_f1=0.5844\n",
      "[department][test] accuracy=0.5785 macro_f1=0.5940 weighted_f1=0.5802\n",
      "Saved baseline metrics: /Users/rawadyared/NLP-IT-Ticket_Triage/results/baselines/department_baseline_metrics.json\n",
      "[urgency][val] accuracy=0.6878 macro_f1=0.6826 weighted_f1=0.6888\n",
      "[urgency][test] accuracy=0.7011 macro_f1=0.6926 weighted_f1=0.7019\n",
      "Saved baseline metrics: /Users/rawadyared/NLP-IT-Ticket_Triage/results/baselines/urgency_baseline_metrics.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Baseline (TF-IDF + Logistic Regression)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "BASELINE_DIR = RESULTS_DIR / \"baselines\"\n",
    "BASELINE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def run_tfidf_logreg_baseline(task_name: str, splits: dict, max_features: int = 80000):\n",
    "    train_df = splits[\"train\"]\n",
    "    val_df = splits[\"val\"]\n",
    "    test_df = splits[\"test\"]\n",
    "\n",
    "    pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\n",
    "                \"tfidf\",\n",
    "                TfidfVectorizer(\n",
    "                    lowercase=True,\n",
    "                    ngram_range=(1, 2),\n",
    "                    max_features=max_features,\n",
    "                    min_df=2,\n",
    "                ),\n",
    "            ),\n",
    "            (\n",
    "                \"clf\",\n",
    "                LogisticRegression(\n",
    "                    max_iter=1000,\n",
    "                    class_weight=\"balanced\",\n",
    "                    random_state=CONFIG[\"seed\"],\n",
    "                    n_jobs=None,\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    pipeline.fit(train_df[\"ticket_text\"], train_df[\"label\"])\n",
    "\n",
    "    metrics_by_split = {}\n",
    "    for split_name, split_df in [(\"val\", val_df), (\"test\", test_df)]:\n",
    "        y_true = split_df[\"label\"].values\n",
    "        y_pred = pipeline.predict(split_df[\"ticket_text\"])\n",
    "\n",
    "        split_metrics = {\n",
    "            \"accuracy\": float(accuracy_score(y_true, y_pred)),\n",
    "            \"macro_f1\": float(f1_score(y_true, y_pred, average=\"macro\", zero_division=0)),\n",
    "            \"weighted_f1\": float(f1_score(y_true, y_pred, average=\"weighted\", zero_division=0)),\n",
    "            \"classification_report\": classification_report(y_true, y_pred, output_dict=True, zero_division=0),\n",
    "            \"confusion_matrix\": confusion_matrix(y_true, y_pred).tolist(),\n",
    "        }\n",
    "        metrics_by_split[split_name] = split_metrics\n",
    "\n",
    "        print(\n",
    "            f\"[{task_name}][{split_name}] \"\n",
    "            f\"accuracy={split_metrics['accuracy']:.4f} \"\n",
    "            f\"macro_f1={split_metrics['macro_f1']:.4f} \"\n",
    "            f\"weighted_f1={split_metrics['weighted_f1']:.4f}\"\n",
    "        )\n",
    "\n",
    "    payload = {\n",
    "        \"task\": task_name,\n",
    "        \"model\": \"tfidf_logistic_regression\",\n",
    "        \"config\": {\n",
    "            \"max_features\": max_features,\n",
    "            \"ngram_range\": [1, 2],\n",
    "            \"min_df\": 2,\n",
    "            \"class_weight\": \"balanced\",\n",
    "            \"seed\": CONFIG[\"seed\"],\n",
    "        },\n",
    "        \"metrics\": metrics_by_split,\n",
    "    }\n",
    "\n",
    "    out_path = BASELINE_DIR / f\"{task_name}_baseline_metrics.json\"\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, ensure_ascii=True, indent=2)\n",
    "    print(f\"Saved baseline metrics: {out_path}\")\n",
    "\n",
    "    return pipeline, payload\n",
    "\n",
    "if \"SPLITS\" not in globals() or not isinstance(SPLITS, dict) or len(SPLITS) == 0:\n",
    "    raise RuntimeError(\"SPLITS not found. Run notebook cells in order through Cell 9 (Train/Val/Test split), then run this baseline cell.\")\n",
    "\n",
    "BASELINE_MODELS = {}\n",
    "BASELINE_RESULTS = {}\n",
    "\n",
    "for task_name in [\"department\", \"urgency\"]:\n",
    "    model, result = run_tfidf_logreg_baseline(task_name=task_name, splits=SPLITS[task_name])\n",
    "    BASELINE_MODELS[task_name] = model\n",
    "    BASELINE_RESULTS[task_name] = result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ad80e5",
   "metadata": {},
   "source": [
    "## Transformer Plan (Multiple Approaches)\n",
    "\n",
    "We compare multiple transformer backbones for **department routing** and select the best on validation macro F1.\n",
    "\n",
    "- Candidate A: `distilroberta-base` (faster)\n",
    "- Candidate B: `bert-base-uncased` (strong baseline)\n",
    "- Selection criterion: best validation macro F1, then test on held-out set\n",
    "\n",
    "Note: Hugging Face model/tokenizer files are downloaded on first use if not cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2db996b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Unnamed: 0', 'ticket_text', 'label', 'label_text'],\n",
      "        num_rows: 18654\n",
      "    })\n",
      "    val: Dataset({\n",
      "        features: ['Unnamed: 0', 'ticket_text', 'label', 'label_text'],\n",
      "        num_rows: 2332\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['Unnamed: 0', 'ticket_text', 'label', 'label_text'],\n",
      "        num_rows: 2332\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Convert to HuggingFace Dataset (department)\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "DEPT_SPLITS = SPLITS[\"department\"]\n",
    "DEPT_LABEL2ID = TASK_DATA[\"department\"][\"label2id\"]\n",
    "DEPT_ID2LABEL = TASK_DATA[\"department\"][\"id2label\"]\n",
    "\n",
    "def to_hf_dataset(split_df: pd.DataFrame, id_col: str) -> Dataset:\n",
    "    keep_cols = [col for col in [id_col, \"ticket_text\", \"label\", \"label_text\"] if col in split_df.columns]\n",
    "    export_df = split_df[keep_cols].copy()\n",
    "    return Dataset.from_pandas(export_df, preserve_index=False)\n",
    "\n",
    "dept_hf_raw = DatasetDict(\n",
    "    {\n",
    "        \"train\": to_hf_dataset(DEPT_SPLITS[\"train\"], CONFIG[\"id_column\"]),\n",
    "        \"val\": to_hf_dataset(DEPT_SPLITS[\"val\"], CONFIG[\"id_column\"]),\n",
    "        \"test\": to_hf_dataset(DEPT_SPLITS[\"test\"], CONFIG[\"id_column\"]),\n",
    "    }\n",
    ")\n",
    "\n",
    "print(dept_hf_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "612f4101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer function ready. First tokenizer load will download files if not cached.\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: Tokenizer + tokenize function\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "def tokenize_department_splits(tokenizer):\n",
    "    text_col = \"ticket_text\"\n",
    "    max_len = CONFIG[\"train\"][\"max_length\"]\n",
    "\n",
    "    def _tokenize(batch):\n",
    "        return tokenizer(batch[text_col], truncation=True, padding=\"max_length\", max_length=max_len)\n",
    "\n",
    "    remove_cols = [c for c in dept_hf_raw[\"train\"].column_names if c != \"label\"]\n",
    "    tokenized = dept_hf_raw.map(_tokenize, batched=True, remove_columns=remove_cols)\n",
    "    tokenized.set_format(type=\"torch\")\n",
    "    return tokenized\n",
    "\n",
    "print(\"Tokenizer function ready. First tokenizer load will download files if not cached.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "351006cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": float(accuracy_score(labels, preds)),\n",
    "        \"macro_f1\": float(f1_score(labels, preds, average=\"macro\", zero_division=0)),\n",
    "        \"weighted_f1\": float(f1_score(labels, preds, average=\"weighted\", zero_division=0)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c26126c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Model init\n",
    "def init_model(model_name: str):\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    print(\"If this is the first run, Hugging Face weights will be downloaded.\")\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=len(DEPT_LABEL2ID),\n",
    "        id2label=DEPT_ID2LABEL,\n",
    "        label2id=DEPT_LABEL2ID,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f3b7631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: Trainer setup\n",
    "def build_trainer(model_name: str, tokenized_ds: DatasetDict, tokenizer, run_dir: Path, num_train_epochs: int) -> Trainer:\n",
    "    model = init_model(model_name)\n",
    "    data_collator = DataCollatorWithPadding(\n",
    "        tokenizer=tokenizer,\n",
    "        pad_to_multiple_of=8 if torch.cuda.is_available() else None,\n",
    "    )\n",
    "\n",
    "    training_args_kwargs = {\n",
    "        \"output_dir\": str(run_dir),\n",
    "        \"learning_rate\": CONFIG[\"train\"][\"learning_rate\"],\n",
    "        \"per_device_train_batch_size\": CONFIG[\"train\"][\"batch_size\"],\n",
    "        \"per_device_eval_batch_size\": CONFIG[\"train\"][\"batch_size\"],\n",
    "        \"num_train_epochs\": num_train_epochs,\n",
    "        \"weight_decay\": CONFIG[\"train\"][\"weight_decay\"],\n",
    "        \"warmup_ratio\": CONFIG[\"train\"][\"warmup_ratio\"],\n",
    "        \"save_strategy\": \"epoch\",\n",
    "        \"load_best_model_at_end\": True,\n",
    "        \"metric_for_best_model\": \"macro_f1\",\n",
    "        \"greater_is_better\": True,\n",
    "        \"save_total_limit\": 2,\n",
    "        \"fp16\": torch.cuda.is_available(),\n",
    "        \"logging_steps\": 50,\n",
    "        \"report_to\": [],\n",
    "        \"seed\": CONFIG[\"seed\"],\n",
    "    }\n",
    "    strategy_key = \"eval_strategy\" if \"eval_strategy\" in TrainingArguments.__init__.__code__.co_varnames else \"evaluation_strategy\"\n",
    "    training_args_kwargs[strategy_key] = \"epoch\"\n",
    "    args = TrainingArguments(**training_args_kwargs)\n",
    "\n",
    "    trainer_kwargs = {\n",
    "        \"model\": model,\n",
    "        \"args\": args,\n",
    "        \"train_dataset\": tokenized_ds[\"train\"],\n",
    "        \"eval_dataset\": tokenized_ds[\"val\"],\n",
    "        \"data_collator\": data_collator,\n",
    "        \"compute_metrics\": compute_metrics,\n",
    "    }\n",
    "    processing_key = \"processing_class\" if \"processing_class\" in Trainer.__init__.__code__.co_varnames else \"tokenizer\"\n",
    "    trainer_kwargs[processing_key] = tokenizer\n",
    "    trainer = Trainer(**trainer_kwargs)\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4b71029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Department] Candidate 1/2: distilroberta-base\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7914750f35c3470a92a433575a13b5f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18654 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76658cf1c12f4686a4436c4b120d2ae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2332 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24d96f6591f94aedb746554545792c37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2332 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: distilroberta-base\n",
      "If this is the first run, Hugging Face weights will be downloaded.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0803061824be4b9e8ff4640eecdfbf89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRobertaForSequenceClassification LOAD REPORT\u001b[0m from: distilroberta-base\n",
      "Key                         | Status     | \n",
      "----------------------------+------------+-\n",
      "lm_head.bias                | UNEXPECTED | \n",
      "roberta.pooler.dense.weight | UNEXPECTED | \n",
      "lm_head.layer_norm.weight   | UNEXPECTED | \n",
      "lm_head.dense.weight        | UNEXPECTED | \n",
      "lm_head.layer_norm.bias     | UNEXPECTED | \n",
      "lm_head.dense.bias          | UNEXPECTED | \n",
      "roberta.pooler.dense.bias   | UNEXPECTED | \n",
      "classifier.dense.weight     | MISSING    | \n",
      "classifier.dense.bias       | MISSING    | \n",
      "classifier.out_proj.bias    | MISSING    | \n",
      "classifier.out_proj.weight  | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n",
      "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1166' max='1166' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1166/1166 08:45, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Weighted F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.632217</td>\n",
       "      <td>1.594233</td>\n",
       "      <td>0.420240</td>\n",
       "      <td>0.252399</td>\n",
       "      <td>0.357002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87fa0fd43a574bb385eee0d9ccb06368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['roberta.embeddings.LayerNorm.weight', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.5.output.LayerNorm.bias'].\n",
      "There were unexpected keys in the checkpoint model loaded: ['roberta.embeddings.LayerNorm.beta', 'roberta.embeddings.LayerNorm.gamma', 'roberta.encoder.layer.0.attention.output.LayerNorm.beta', 'roberta.encoder.layer.0.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.0.output.LayerNorm.beta', 'roberta.encoder.layer.0.output.LayerNorm.gamma', 'roberta.encoder.layer.1.attention.output.LayerNorm.beta', 'roberta.encoder.layer.1.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.1.output.LayerNorm.beta', 'roberta.encoder.layer.1.output.LayerNorm.gamma', 'roberta.encoder.layer.2.attention.output.LayerNorm.beta', 'roberta.encoder.layer.2.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.2.output.LayerNorm.beta', 'roberta.encoder.layer.2.output.LayerNorm.gamma', 'roberta.encoder.layer.3.attention.output.LayerNorm.beta', 'roberta.encoder.layer.3.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.3.output.LayerNorm.beta', 'roberta.encoder.layer.3.output.LayerNorm.gamma', 'roberta.encoder.layer.4.attention.output.LayerNorm.beta', 'roberta.encoder.layer.4.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.4.output.LayerNorm.beta', 'roberta.encoder.layer.4.output.LayerNorm.gamma', 'roberta.encoder.layer.5.attention.output.LayerNorm.beta', 'roberta.encoder.layer.5.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.5.output.LayerNorm.beta', 'roberta.encoder.layer.5.output.LayerNorm.gamma'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='146' max='146' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [146/146 00:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation macro F1: 0.2524\n",
      "\n",
      "[Department] Candidate 2/2: bert-base-uncased\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbae856541d84996a5e5b44af3de2376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87a2d8fbe28f4efc87666f8ae28499fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bce46c559b24a2091f4084812aef6ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9db33fa7020c412a87e95a2af8bafdc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb47652045424ee6941d83f52c2eb0cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18654 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd2ef03b9cb4b33aa93b28c04dddf69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2332 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "805d785560094d1c988297555ff2a23e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2332 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: bert-base-uncased\n",
      "If this is the first run, Hugging Face weights will be downloaded.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cc8dde07174483696e497de2d4d318b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ff7dcd865584bdbb67db57ac597c321",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertForSequenceClassification LOAD REPORT\u001b[0m from: bert-base-uncased\n",
      "Key                                        | Status     | \n",
      "-------------------------------------------+------------+-\n",
      "cls.seq_relationship.weight                | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED | \n",
      "cls.predictions.bias                       | UNEXPECTED | \n",
      "classifier.bias                            | MISSING    | \n",
      "classifier.weight                          | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n",
      "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1166' max='1166' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1166/1166 15:52, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Weighted F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.638387</td>\n",
       "      <td>1.596577</td>\n",
       "      <td>0.413808</td>\n",
       "      <td>0.274538</td>\n",
       "      <td>0.356114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "769f17554ebc4b9b8c6eb0677c83ce98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias'].\n",
      "There were unexpected keys in the checkpoint model loaded: ['bert.embeddings.LayerNorm.beta', 'bert.embeddings.LayerNorm.gamma', 'bert.encoder.layer.0.attention.output.LayerNorm.beta', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma', 'bert.encoder.layer.0.output.LayerNorm.beta', 'bert.encoder.layer.0.output.LayerNorm.gamma', 'bert.encoder.layer.1.attention.output.LayerNorm.beta', 'bert.encoder.layer.1.attention.output.LayerNorm.gamma', 'bert.encoder.layer.1.output.LayerNorm.beta', 'bert.encoder.layer.1.output.LayerNorm.gamma', 'bert.encoder.layer.2.attention.output.LayerNorm.beta', 'bert.encoder.layer.2.attention.output.LayerNorm.gamma', 'bert.encoder.layer.2.output.LayerNorm.beta', 'bert.encoder.layer.2.output.LayerNorm.gamma', 'bert.encoder.layer.3.attention.output.LayerNorm.beta', 'bert.encoder.layer.3.attention.output.LayerNorm.gamma', 'bert.encoder.layer.3.output.LayerNorm.beta', 'bert.encoder.layer.3.output.LayerNorm.gamma', 'bert.encoder.layer.4.attention.output.LayerNorm.beta', 'bert.encoder.layer.4.attention.output.LayerNorm.gamma', 'bert.encoder.layer.4.output.LayerNorm.beta', 'bert.encoder.layer.4.output.LayerNorm.gamma', 'bert.encoder.layer.5.attention.output.LayerNorm.beta', 'bert.encoder.layer.5.attention.output.LayerNorm.gamma', 'bert.encoder.layer.5.output.LayerNorm.beta', 'bert.encoder.layer.5.output.LayerNorm.gamma', 'bert.encoder.layer.6.attention.output.LayerNorm.beta', 'bert.encoder.layer.6.attention.output.LayerNorm.gamma', 'bert.encoder.layer.6.output.LayerNorm.beta', 'bert.encoder.layer.6.output.LayerNorm.gamma', 'bert.encoder.layer.7.attention.output.LayerNorm.beta', 'bert.encoder.layer.7.attention.output.LayerNorm.gamma', 'bert.encoder.layer.7.output.LayerNorm.beta', 'bert.encoder.layer.7.output.LayerNorm.gamma', 'bert.encoder.layer.8.attention.output.LayerNorm.beta', 'bert.encoder.layer.8.attention.output.LayerNorm.gamma', 'bert.encoder.layer.8.output.LayerNorm.beta', 'bert.encoder.layer.8.output.LayerNorm.gamma', 'bert.encoder.layer.9.attention.output.LayerNorm.beta', 'bert.encoder.layer.9.attention.output.LayerNorm.gamma', 'bert.encoder.layer.9.output.LayerNorm.beta', 'bert.encoder.layer.9.output.LayerNorm.gamma', 'bert.encoder.layer.10.attention.output.LayerNorm.beta', 'bert.encoder.layer.10.attention.output.LayerNorm.gamma', 'bert.encoder.layer.10.output.LayerNorm.beta', 'bert.encoder.layer.10.output.LayerNorm.gamma', 'bert.encoder.layer.11.attention.output.LayerNorm.beta', 'bert.encoder.layer.11.attention.output.LayerNorm.gamma', 'bert.encoder.layer.11.output.LayerNorm.beta', 'bert.encoder.layer.11.output.LayerNorm.gamma'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='146' max='146' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [146/146 00:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation macro F1: 0.2745\n",
      "\n",
      "Candidate comparison complete.\n",
      "        model_name  candidate_epochs  val_accuracy  val_macro_f1  val_weighted_f1\n",
      " bert-base-uncased                 1      0.413808      0.274538         0.356114\n",
      "distilroberta-base                 1      0.420240      0.252399         0.357002\n",
      "\n",
      "Retraining best candidate (bert-base-uncased) for full epochs: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9db2b6e0b2d24873b26affb3243213d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18654 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3f36981e977443e834313e344de9b2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2332 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29fab3851e364ef28650f5ae60fef400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2332 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: bert-base-uncased\n",
      "If this is the first run, Hugging Face weights will be downloaded.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c5b768c27e04e5f94e08bc1b29afaff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertForSequenceClassification LOAD REPORT\u001b[0m from: bert-base-uncased\n",
      "Key                                        | Status     | \n",
      "-------------------------------------------+------------+-\n",
      "cls.seq_relationship.weight                | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED | \n",
      "cls.predictions.bias                       | UNEXPECTED | \n",
      "classifier.bias                            | MISSING    | \n",
      "classifier.weight                          | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n",
      "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='3498' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  30/3498 00:22 < 45:42, 1.26 it/s, Epoch 0.02/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 79\u001b[39m\n\u001b[32m     71\u001b[39m tokenized_ds = tokenize_department_splits(tokenizer)\n\u001b[32m     72\u001b[39m trainer = build_trainer(\n\u001b[32m     73\u001b[39m     model_name=best_model_name,\n\u001b[32m     74\u001b[39m     tokenized_ds=tokenized_ds,\n\u001b[32m   (...)\u001b[39m\u001b[32m     77\u001b[39m     num_train_epochs=full_epochs,\n\u001b[32m     78\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m DEPT_BEST_TRAINER = trainer\n\u001b[32m     81\u001b[39m DEPT_BEST_TOKENIZER = tokenizer\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NLP-IT-Ticket_Triage/.venv/lib/python3.12/site-packages/transformers/trainer.py:2170\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2168\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2169\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2170\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2171\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2172\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2173\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2174\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2175\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NLP-IT-Ticket_Triage/.venv/lib/python3.12/site-packages/transformers/trainer.py:2537\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2530\u001b[39m context = (\n\u001b[32m   2531\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2532\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2533\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2534\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2535\u001b[39m )\n\u001b[32m   2536\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2537\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2539\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2540\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2541\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2542\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2543\u001b[39m ):\n\u001b[32m   2544\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2545\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NLP-IT-Ticket_Triage/.venv/lib/python3.12/site-packages/transformers/trainer.py:3838\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   3835\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   3836\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3838\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3840\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NLP-IT-Ticket_Triage/.venv/lib/python3.12/site-packages/accelerate/accelerator.py:2852\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2850\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2851\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2852\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NLP-IT-Ticket_Triage/.venv/lib/python3.12/site-packages/torch/_tensor.py:630\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    620\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    621\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    622\u001b[39m         Tensor.backward,\n\u001b[32m    623\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    628\u001b[39m         inputs=inputs,\n\u001b[32m    629\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NLP-IT-Ticket_Triage/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:364\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    359\u001b[39m     retain_graph = create_graph\n\u001b[32m    361\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    362\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    363\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NLP-IT-Ticket_Triage/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:865\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    863\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    864\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m865\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    866\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    869\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Cell 18: Train department classifier (candidate comparison + best model save)\n",
    "DEPT_MODELS_DIR = MODELS_DIR / \"department_model\"\n",
    "DEPT_MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "candidate_models = (\n",
    "    CONFIG[\"candidate_models\"][\"department\"]\n",
    "    if CONFIG[\"experiment\"][\"run_candidate_search\"]\n",
    "    else [CONFIG[\"model_names\"][\"department\"]]\n",
    ")\n",
    "\n",
    "candidate_epochs = CONFIG[\"experiment\"][\"candidate_epochs\"]\n",
    "full_epochs = CONFIG[\"train\"][\"epochs\"]\n",
    "\n",
    "DEPT_EXPERIMENTS = []\n",
    "DEPT_BEST = None\n",
    "DEPT_BEST_TRAINER = None\n",
    "DEPT_BEST_TOKENIZER = None\n",
    "DEPT_BEST_TOKENIZED_DS = None\n",
    "\n",
    "for idx, model_name in enumerate(candidate_models, start=1):\n",
    "    safe_name = model_name.replace(\"/\", \"_\")\n",
    "    run_dir = DEPT_MODELS_DIR / f\"candidate_{idx}_{safe_name}\"\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"\\n[Department] Candidate {idx}/{len(candidate_models)}: {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    tokenized_ds = tokenize_department_splits(tokenizer)\n",
    "\n",
    "    trainer = build_trainer(\n",
    "        model_name=model_name,\n",
    "        tokenized_ds=tokenized_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        run_dir=run_dir,\n",
    "        num_train_epochs=candidate_epochs,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    val_metrics = trainer.evaluate(tokenized_ds[\"val\"])\n",
    "    val_macro_f1 = float(val_metrics.get(\"eval_macro_f1\", -1.0))\n",
    "\n",
    "    experiment_row = {\n",
    "        \"model_name\": model_name,\n",
    "        \"candidate_epochs\": candidate_epochs,\n",
    "        \"val_accuracy\": float(val_metrics.get(\"eval_accuracy\", 0.0)),\n",
    "        \"val_macro_f1\": val_macro_f1,\n",
    "        \"val_weighted_f1\": float(val_metrics.get(\"eval_weighted_f1\", 0.0)),\n",
    "    }\n",
    "    DEPT_EXPERIMENTS.append(experiment_row)\n",
    "    print(f\"Validation macro F1: {val_macro_f1:.4f}\")\n",
    "\n",
    "    if (DEPT_BEST is None) or (val_macro_f1 > DEPT_BEST[\"val_macro_f1\"]):\n",
    "        DEPT_BEST = experiment_row\n",
    "        DEPT_BEST_TRAINER = trainer\n",
    "        DEPT_BEST_TOKENIZER = tokenizer\n",
    "        DEPT_BEST_TOKENIZED_DS = tokenized_ds\n",
    "\n",
    "print(\"\\nCandidate comparison complete.\")\n",
    "print(pd.DataFrame(DEPT_EXPERIMENTS).sort_values(\"val_macro_f1\", ascending=False).to_string(index=False))\n",
    "\n",
    "best_model_name = DEPT_BEST[\"model_name\"]\n",
    "best_model_dir = DEPT_MODELS_DIR / \"best\"\n",
    "best_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Optional full retrain on best candidate with configured epochs.\n",
    "if full_epochs > candidate_epochs:\n",
    "    print(\n",
    "        f\"\\nRetraining best candidate ({best_model_name}) for full epochs: \"\n",
    "        f\"{full_epochs}\"\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(best_model_name, use_fast=True)\n",
    "    tokenized_ds = tokenize_department_splits(tokenizer)\n",
    "    trainer = build_trainer(\n",
    "        model_name=best_model_name,\n",
    "        tokenized_ds=tokenized_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        run_dir=best_model_dir,\n",
    "        num_train_epochs=full_epochs,\n",
    "    )\n",
    "    trainer.train()\n",
    "    DEPT_BEST_TRAINER = trainer\n",
    "    DEPT_BEST_TOKENIZER = tokenizer\n",
    "    DEPT_BEST_TOKENIZED_DS = tokenized_ds\n",
    "\n",
    "DEPT_BEST_TRAINER.save_model(str(best_model_dir))\n",
    "DEPT_BEST_TOKENIZER.save_pretrained(str(best_model_dir))\n",
    "print(f\"Saved best department model to: {best_model_dir}\")\n",
    "\n",
    "dept_experiment_path = RESULTS_DIR / \"department_model_selection.json\"\n",
    "with open(dept_experiment_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"selected_model\": best_model_name,\n",
    "            \"candidate_results\": DEPT_EXPERIMENTS,\n",
    "            \"candidate_epochs\": candidate_epochs,\n",
    "            \"full_epochs\": full_epochs,\n",
    "        },\n",
    "        f,\n",
    "        ensure_ascii=True,\n",
    "        indent=2,\n",
    "    )\n",
    "print(f\"Saved model-selection report: {dept_experiment_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a681816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 19: Evaluate department classifier (test)\n",
    "dept_test_output = DEPT_BEST_TRAINER.predict(DEPT_BEST_TOKENIZED_DS[\"test\"])\n",
    "dept_test_logits = dept_test_output.predictions\n",
    "dept_test_probs = torch.softmax(torch.tensor(dept_test_logits), dim=-1).cpu().numpy()\n",
    "\n",
    "y_true = DEPT_SPLITS[\"test\"][\"label\"].to_numpy()\n",
    "y_pred = dept_test_probs.argmax(axis=1)\n",
    "conf = dept_test_probs.max(axis=1)\n",
    "\n",
    "dept_test_metrics = {\n",
    "    \"accuracy\": float(accuracy_score(y_true, y_pred)),\n",
    "    \"macro_f1\": float(f1_score(y_true, y_pred, average=\"macro\", zero_division=0)),\n",
    "    \"weighted_f1\": float(f1_score(y_true, y_pred, average=\"weighted\", zero_division=0)),\n",
    "    \"classification_report\": classification_report(y_true, y_pred, output_dict=True, zero_division=0),\n",
    "    \"confusion_matrix\": confusion_matrix(y_true, y_pred).tolist(),\n",
    "}\n",
    "\n",
    "print(\n",
    "    f\"Department transformer test -> \"\n",
    "    f\"accuracy={dept_test_metrics['accuracy']:.4f}, \"\n",
    "    f\"macro_f1={dept_test_metrics['macro_f1']:.4f}, \"\n",
    "    f\"weighted_f1={dept_test_metrics['weighted_f1']:.4f}\"\n",
    ")\n",
    "\n",
    "metrics_payload = {\n",
    "    \"task\": \"department\",\n",
    "    \"selected_model\": DEPT_BEST[\"model_name\"],\n",
    "    \"test_metrics\": dept_test_metrics,\n",
    "}\n",
    "dept_metrics_path = RESULTS_DIR / \"department_transformer_metrics.json\"\n",
    "with open(dept_metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metrics_payload, f, ensure_ascii=True, indent=2)\n",
    "print(f\"Saved test metrics: {dept_metrics_path}\")\n",
    "\n",
    "id_col = CONFIG[\"id_column\"]\n",
    "pred_df = DEPT_SPLITS[\"test\"][[id_col, \"ticket_text\", \"label\", \"label_text\"]].copy()\n",
    "pred_df[\"pred_label_id\"] = y_pred\n",
    "pred_df[\"pred_label\"] = pred_df[\"pred_label_id\"].map(DEPT_ID2LABEL)\n",
    "pred_df[\"confidence\"] = conf\n",
    "\n",
    "pred_out_path = RESULTS_DIR / \"department_test_predictions.csv\"\n",
    "pred_df.to_csv(pred_out_path, index=False)\n",
    "print(f\"Saved test predictions: {pred_out_path}\")\n",
    "display(pred_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ddfa7c",
   "metadata": {},
   "source": [
    "## Urgency Transformer (Multiple Approaches)\n",
    "\n",
    "We now apply the same multi-approach process to urgency/priority:\n",
    "\n",
    "- Candidate A: `distilroberta-base`\n",
    "- Candidate B: `bert-base-uncased`\n",
    "- Model selection: best validation macro F1\n",
    "\n",
    "Note: Hugging Face tokenizer/model files are downloaded on first run if not cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512509f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 20: Repeat transformer pipeline for urgency/priority (multi-approach)\n",
    "URGENCY_SPLITS = SPLITS[\"urgency\"]\n",
    "URGENCY_LABEL2ID = TASK_DATA[\"urgency\"][\"label2id\"]\n",
    "URGENCY_ID2LABEL = TASK_DATA[\"urgency\"][\"id2label\"]\n",
    "\n",
    "URGENCY_MODELS_DIR = MODELS_DIR / \"urgency_model\"\n",
    "URGENCY_MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def to_hf_dataset_urgency(split_df: pd.DataFrame, id_col: str) -> Dataset:\n",
    "    keep_cols = [col for col in [id_col, \"ticket_text\", \"label\", \"label_text\"] if col in split_df.columns]\n",
    "    return Dataset.from_pandas(split_df[keep_cols].copy(), preserve_index=False)\n",
    "\n",
    "urgency_hf_raw = DatasetDict(\n",
    "    {\n",
    "        \"train\": to_hf_dataset_urgency(URGENCY_SPLITS[\"train\"], CONFIG[\"id_column\"]),\n",
    "        \"val\": to_hf_dataset_urgency(URGENCY_SPLITS[\"val\"], CONFIG[\"id_column\"]),\n",
    "        \"test\": to_hf_dataset_urgency(URGENCY_SPLITS[\"test\"], CONFIG[\"id_column\"]),\n",
    "    }\n",
    ")\n",
    "\n",
    "def tokenize_urgency_splits(tokenizer):\n",
    "    def _tokenize(batch):\n",
    "        return tokenizer(\n",
    "            batch[\"ticket_text\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=CONFIG[\"train\"][\"max_length\"],\n",
    "        )\n",
    "\n",
    "    remove_cols = [c for c in urgency_hf_raw[\"train\"].column_names if c != \"label\"]\n",
    "    tokenized = urgency_hf_raw.map(_tokenize, batched=True, remove_columns=remove_cols)\n",
    "    tokenized.set_format(type=\"torch\")\n",
    "    return tokenized\n",
    "\n",
    "def init_urgency_model(model_name: str):\n",
    "    print(f\"Loading urgency model: {model_name}\")\n",
    "    print(\"If first run, Hugging Face downloads tokenizer and model weights.\")\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=len(URGENCY_LABEL2ID),\n",
    "        id2label=URGENCY_ID2LABEL,\n",
    "        label2id=URGENCY_LABEL2ID,\n",
    "    )\n",
    "\n",
    "def build_urgency_trainer(model_name: str, tokenized_ds: DatasetDict, tokenizer, run_dir: Path, num_train_epochs: int) -> Trainer:\n",
    "    model = init_urgency_model(model_name)\n",
    "    data_collator = DataCollatorWithPadding(\n",
    "        tokenizer=tokenizer,\n",
    "        pad_to_multiple_of=8 if torch.cuda.is_available() else None,\n",
    "    )\n",
    "\n",
    "    training_args_kwargs = {\n",
    "        \"output_dir\": str(run_dir),\n",
    "        \"learning_rate\": CONFIG[\"train\"][\"learning_rate\"],\n",
    "        \"per_device_train_batch_size\": CONFIG[\"train\"][\"batch_size\"],\n",
    "        \"per_device_eval_batch_size\": CONFIG[\"train\"][\"batch_size\"],\n",
    "        \"num_train_epochs\": num_train_epochs,\n",
    "        \"weight_decay\": CONFIG[\"train\"][\"weight_decay\"],\n",
    "        \"warmup_ratio\": CONFIG[\"train\"][\"warmup_ratio\"],\n",
    "        \"save_strategy\": \"epoch\",\n",
    "        \"load_best_model_at_end\": True,\n",
    "        \"metric_for_best_model\": \"macro_f1\",\n",
    "        \"greater_is_better\": True,\n",
    "        \"save_total_limit\": 2,\n",
    "        \"fp16\": torch.cuda.is_available(),\n",
    "        \"logging_steps\": 50,\n",
    "        \"report_to\": [],\n",
    "        \"seed\": CONFIG[\"seed\"],\n",
    "    }\n",
    "    strategy_key = \"eval_strategy\" if \"eval_strategy\" in TrainingArguments.__init__.__code__.co_varnames else \"evaluation_strategy\"\n",
    "    training_args_kwargs[strategy_key] = \"epoch\"\n",
    "    args = TrainingArguments(**training_args_kwargs)\n",
    "\n",
    "    trainer_kwargs = {\n",
    "        \"model\": model,\n",
    "        \"args\": args,\n",
    "        \"train_dataset\": tokenized_ds[\"train\"],\n",
    "        \"eval_dataset\": tokenized_ds[\"val\"],\n",
    "        \"data_collator\": data_collator,\n",
    "        \"compute_metrics\": compute_metrics,\n",
    "    }\n",
    "    processing_key = \"processing_class\" if \"processing_class\" in Trainer.__init__.__code__.co_varnames else \"tokenizer\"\n",
    "    trainer_kwargs[processing_key] = tokenizer\n",
    "    return Trainer(**trainer_kwargs)\n",
    "\n",
    "urgency_candidate_models = (\n",
    "    CONFIG[\"candidate_models\"][\"urgency\"]\n",
    "    if CONFIG[\"experiment\"][\"run_candidate_search\"]\n",
    "    else [CONFIG[\"model_names\"][\"urgency\"]]\n",
    ")\n",
    "\n",
    "urgency_candidate_epochs = CONFIG[\"experiment\"][\"candidate_epochs\"]\n",
    "urgency_full_epochs = CONFIG[\"train\"][\"epochs\"]\n",
    "\n",
    "URGENCY_EXPERIMENTS = []\n",
    "URGENCY_BEST = None\n",
    "URGENCY_BEST_TRAINER = None\n",
    "URGENCY_BEST_TOKENIZER = None\n",
    "URGENCY_BEST_TOKENIZED_DS = None\n",
    "\n",
    "for idx, model_name in enumerate(urgency_candidate_models, start=1):\n",
    "    safe_name = model_name.replace(\"/\", \"_\")\n",
    "    run_dir = URGENCY_MODELS_DIR / f\"candidate_{idx}_{safe_name}\"\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"\\n[Urgency] Candidate {idx}/{len(urgency_candidate_models)}: {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    tokenized_ds = tokenize_urgency_splits(tokenizer)\n",
    "\n",
    "    trainer = build_urgency_trainer(\n",
    "        model_name=model_name,\n",
    "        tokenized_ds=tokenized_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        run_dir=run_dir,\n",
    "        num_train_epochs=urgency_candidate_epochs,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    val_metrics = trainer.evaluate(tokenized_ds[\"val\"])\n",
    "    val_macro_f1 = float(val_metrics.get(\"eval_macro_f1\", -1.0))\n",
    "\n",
    "    experiment_row = {\n",
    "        \"model_name\": model_name,\n",
    "        \"candidate_epochs\": urgency_candidate_epochs,\n",
    "        \"val_accuracy\": float(val_metrics.get(\"eval_accuracy\", 0.0)),\n",
    "        \"val_macro_f1\": val_macro_f1,\n",
    "        \"val_weighted_f1\": float(val_metrics.get(\"eval_weighted_f1\", 0.0)),\n",
    "    }\n",
    "    URGENCY_EXPERIMENTS.append(experiment_row)\n",
    "    print(f\"Validation macro F1: {val_macro_f1:.4f}\")\n",
    "\n",
    "    if (URGENCY_BEST is None) or (val_macro_f1 > URGENCY_BEST[\"val_macro_f1\"]):\n",
    "        URGENCY_BEST = experiment_row\n",
    "        URGENCY_BEST_TRAINER = trainer\n",
    "        URGENCY_BEST_TOKENIZER = tokenizer\n",
    "        URGENCY_BEST_TOKENIZED_DS = tokenized_ds\n",
    "\n",
    "print(\"\\nUrgency candidate comparison complete.\")\n",
    "print(pd.DataFrame(URGENCY_EXPERIMENTS).sort_values(\"val_macro_f1\", ascending=False).to_string(index=False))\n",
    "\n",
    "urgency_best_model_name = URGENCY_BEST[\"model_name\"]\n",
    "urgency_best_model_dir = URGENCY_MODELS_DIR / \"best\"\n",
    "urgency_best_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if urgency_full_epochs > urgency_candidate_epochs:\n",
    "    print(\n",
    "        f\"\\nRetraining best urgency candidate ({urgency_best_model_name}) for full epochs: \"\n",
    "        f\"{urgency_full_epochs}\"\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(urgency_best_model_name, use_fast=True)\n",
    "    tokenized_ds = tokenize_urgency_splits(tokenizer)\n",
    "    trainer = build_urgency_trainer(\n",
    "        model_name=urgency_best_model_name,\n",
    "        tokenized_ds=tokenized_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        run_dir=urgency_best_model_dir,\n",
    "        num_train_epochs=urgency_full_epochs,\n",
    "    )\n",
    "    trainer.train()\n",
    "    URGENCY_BEST_TRAINER = trainer\n",
    "    URGENCY_BEST_TOKENIZER = tokenizer\n",
    "    URGENCY_BEST_TOKENIZED_DS = tokenized_ds\n",
    "\n",
    "URGENCY_BEST_TRAINER.save_model(str(urgency_best_model_dir))\n",
    "URGENCY_BEST_TOKENIZER.save_pretrained(str(urgency_best_model_dir))\n",
    "print(f\"Saved best urgency model to: {urgency_best_model_dir}\")\n",
    "\n",
    "urgency_selection_path = RESULTS_DIR / \"urgency_model_selection.json\"\n",
    "with open(urgency_selection_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"selected_model\": urgency_best_model_name,\n",
    "            \"candidate_results\": URGENCY_EXPERIMENTS,\n",
    "            \"candidate_epochs\": urgency_candidate_epochs,\n",
    "            \"full_epochs\": urgency_full_epochs,\n",
    "        },\n",
    "        f,\n",
    "        ensure_ascii=True,\n",
    "        indent=2,\n",
    "    )\n",
    "print(f\"Saved urgency model-selection report: {urgency_selection_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c896f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 20 (cont.): Evaluate urgency classifier (test)\n",
    "urgency_test_output = URGENCY_BEST_TRAINER.predict(URGENCY_BEST_TOKENIZED_DS[\"test\"])\n",
    "urgency_test_logits = urgency_test_output.predictions\n",
    "urgency_test_probs = torch.softmax(torch.tensor(urgency_test_logits), dim=-1).cpu().numpy()\n",
    "\n",
    "y_true = URGENCY_SPLITS[\"test\"][\"label\"].to_numpy()\n",
    "y_pred = urgency_test_probs.argmax(axis=1)\n",
    "conf = urgency_test_probs.max(axis=1)\n",
    "\n",
    "urgency_test_metrics = {\n",
    "    \"accuracy\": float(accuracy_score(y_true, y_pred)),\n",
    "    \"macro_f1\": float(f1_score(y_true, y_pred, average=\"macro\", zero_division=0)),\n",
    "    \"weighted_f1\": float(f1_score(y_true, y_pred, average=\"weighted\", zero_division=0)),\n",
    "    \"classification_report\": classification_report(y_true, y_pred, output_dict=True, zero_division=0),\n",
    "    \"confusion_matrix\": confusion_matrix(y_true, y_pred).tolist(),\n",
    "}\n",
    "\n",
    "print(\n",
    "    f\"Urgency transformer test -> \"\n",
    "    f\"accuracy={urgency_test_metrics['accuracy']:.4f}, \"\n",
    "    f\"macro_f1={urgency_test_metrics['macro_f1']:.4f}, \"\n",
    "    f\"weighted_f1={urgency_test_metrics['weighted_f1']:.4f}\"\n",
    ")\n",
    "\n",
    "urgency_metrics_payload = {\n",
    "    \"task\": \"urgency\",\n",
    "    \"selected_model\": URGENCY_BEST[\"model_name\"],\n",
    "    \"test_metrics\": urgency_test_metrics,\n",
    "}\n",
    "urgency_metrics_path = RESULTS_DIR / \"urgency_transformer_metrics.json\"\n",
    "with open(urgency_metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(urgency_metrics_payload, f, ensure_ascii=True, indent=2)\n",
    "print(f\"Saved urgency test metrics: {urgency_metrics_path}\")\n",
    "\n",
    "id_col = CONFIG[\"id_column\"]\n",
    "urgency_pred_df = URGENCY_SPLITS[\"test\"][[id_col, \"ticket_text\", \"label\", \"label_text\"]].copy()\n",
    "urgency_pred_df[\"pred_label_id\"] = y_pred\n",
    "urgency_pred_df[\"pred_label\"] = urgency_pred_df[\"pred_label_id\"].map(URGENCY_ID2LABEL)\n",
    "urgency_pred_df[\"confidence\"] = conf\n",
    "\n",
    "urgency_pred_out_path = RESULTS_DIR / \"urgency_test_predictions.csv\"\n",
    "urgency_pred_df.to_csv(urgency_pred_out_path, index=False)\n",
    "print(f\"Saved urgency test predictions: {urgency_pred_out_path}\")\n",
    "display(urgency_pred_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9af927",
   "metadata": {},
   "source": [
    "## Tags and Summary (Prototype Inference)\n",
    "\n",
    "Approach:\n",
    "\n",
    "- **Tags**: extractive keyphrases with YAKE (lightweight, stable, CPU-friendly)\n",
    "- **Summary**: abstractive summary with `t5-small`\n",
    "\n",
    "Download note:\n",
    "\n",
    "- `t5-small` is downloaded from Hugging Face on first run if not cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4265d3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 22: Tags extraction module (YAKE)\n",
    "import yake\n",
    "\n",
    "YAKE_CONFIG = {\n",
    "    \"lan\": \"en\",\n",
    "    \"n\": 3,\n",
    "    \"dedupLim\": 0.9,\n",
    "    \"dedupFunc\": \"seqm\",\n",
    "    \"windowsSize\": 1,\n",
    "}\n",
    "\n",
    "def extract_tags(text: str, top_k: int = 5):\n",
    "    clean_text = normalize_ticket_text(text)\n",
    "    if not clean_text:\n",
    "        return []\n",
    "\n",
    "    extractor = yake.KeywordExtractor(top=top_k * 2, **YAKE_CONFIG)\n",
    "    keywords = extractor.extract_keywords(clean_text)\n",
    "\n",
    "    tags = []\n",
    "    seen = set()\n",
    "    for phrase, _score in keywords:\n",
    "        candidate = normalize_ticket_text(phrase).lower()\n",
    "        if len(candidate) < 3:\n",
    "            continue\n",
    "        if candidate in seen:\n",
    "            continue\n",
    "        seen.add(candidate)\n",
    "        tags.append(candidate)\n",
    "        if len(tags) >= top_k:\n",
    "            break\n",
    "\n",
    "    return tags\n",
    "\n",
    "print(\"YAKE tag extractor ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb387de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 23: Summary module (t5-small)\n",
    "from transformers import pipeline\n",
    "\n",
    "SUMMARIZER = None\n",
    "\n",
    "def get_summarizer():\n",
    "    global SUMMARIZER\n",
    "    if SUMMARIZER is None:\n",
    "        model_name = CONFIG[\"model_names\"][\"summary\"]\n",
    "        print(f\"Loading summarizer: {model_name}\")\n",
    "        print(\"If first run, Hugging Face will download summarizer files.\")\n",
    "        device = 0 if torch.cuda.is_available() else -1\n",
    "        SUMMARIZER = pipeline(\"summarization\", model=model_name, tokenizer=model_name, device=device)\n",
    "    return SUMMARIZER\n",
    "\n",
    "def summarize_text(text: str, min_len: int = 12, max_len: int = 60):\n",
    "    clean_text = normalize_ticket_text(text)\n",
    "    if not clean_text:\n",
    "        return \"\"\n",
    "\n",
    "    if len(clean_text.split()) < 25:\n",
    "        return clean_text\n",
    "\n",
    "    clean_text = clean_text[:3000]\n",
    "    summarizer = get_summarizer()\n",
    "    prefixed = f\"summarize: {clean_text}\"\n",
    "\n",
    "    output = summarizer(\n",
    "        prefixed,\n",
    "        max_length=max_len,\n",
    "        min_length=min_len,\n",
    "        do_sample=False,\n",
    "        truncation=True,\n",
    "    )\n",
    "    return normalize_ticket_text(output[0][\"summary_text\"])\n",
    "\n",
    "print(\"Summary module ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb11acfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 24: Unified inference function -> JSON\n",
    "from typing import Dict\n",
    "\n",
    "CLASSIFIER_CACHE = {}\n",
    "\n",
    "def _get_label_maps(task_name: str):\n",
    "    return TASK_DATA[task_name][\"label2id\"], TASK_DATA[task_name][\"id2label\"]\n",
    "\n",
    "def _best_model_dir(task_name: str) -> Path:\n",
    "    return MODELS_DIR / f\"{task_name}_model\" / \"best\"\n",
    "\n",
    "def _get_transformer_runtime(task_name: str):\n",
    "    cache_key = f\"transformer::{task_name}\"\n",
    "    if cache_key in CLASSIFIER_CACHE:\n",
    "        return CLASSIFIER_CACHE[cache_key]\n",
    "\n",
    "    trainer_var = \"DEPT_BEST_TRAINER\" if task_name == \"department\" else \"URGENCY_BEST_TRAINER\"\n",
    "    tokenizer_var = \"DEPT_BEST_TOKENIZER\" if task_name == \"department\" else \"URGENCY_BEST_TOKENIZER\"\n",
    "    if trainer_var in globals() and tokenizer_var in globals() and globals()[trainer_var] is not None:\n",
    "        runtime = {\n",
    "            \"mode\": \"in_memory_transformer\",\n",
    "            \"model\": globals()[trainer_var].model,\n",
    "            \"tokenizer\": globals()[tokenizer_var],\n",
    "        }\n",
    "        CLASSIFIER_CACHE[cache_key] = runtime\n",
    "        return runtime\n",
    "\n",
    "    model_dir = _best_model_dir(task_name)\n",
    "    if model_dir.exists():\n",
    "        runtime = {\n",
    "            \"mode\": \"disk_transformer\",\n",
    "            \"model\": AutoModelForSequenceClassification.from_pretrained(str(model_dir)),\n",
    "            \"tokenizer\": AutoTokenizer.from_pretrained(str(model_dir), use_fast=True),\n",
    "        }\n",
    "        CLASSIFIER_CACHE[cache_key] = runtime\n",
    "        return runtime\n",
    "\n",
    "    if \"BASELINE_MODELS\" in globals() and task_name in BASELINE_MODELS:\n",
    "        runtime = {\n",
    "            \"mode\": \"baseline\",\n",
    "            \"model\": BASELINE_MODELS[task_name],\n",
    "            \"tokenizer\": None,\n",
    "        }\n",
    "        CLASSIFIER_CACHE[cache_key] = runtime\n",
    "        return runtime\n",
    "\n",
    "    raise RuntimeError(\n",
    "        f\"No model available for task '{task_name}'. Run training cells first \"\n",
    "        f\"or ensure baseline cell executed.\"\n",
    "    )\n",
    "\n",
    "def _predict_label(task_name: str, text: str):\n",
    "    _label2id, id2label = _get_label_maps(task_name)\n",
    "    runtime = _get_transformer_runtime(task_name)\n",
    "\n",
    "    if runtime[\"mode\"] == \"baseline\":\n",
    "        probs = runtime[\"model\"].predict_proba([text])[0]\n",
    "        pred_id = int(np.argmax(probs))\n",
    "        conf = float(probs[pred_id])\n",
    "    else:\n",
    "        model = runtime[\"model\"]\n",
    "        tokenizer = runtime[\"tokenizer\"]\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        enc = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=CONFIG[\"train\"][\"max_length\"],\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        enc = {k: v.to(device) for k, v in enc.items()}\n",
    "        with torch.no_grad():\n",
    "            logits = model(**enc).logits\n",
    "            probs = torch.softmax(logits, dim=-1).squeeze(0).detach().cpu().numpy()\n",
    "        pred_id = int(np.argmax(probs))\n",
    "        conf = float(probs[pred_id])\n",
    "\n",
    "    label = id2label[pred_id]\n",
    "    return {\n",
    "        \"label\": label,\n",
    "        \"confidence\": round(conf, 6),\n",
    "    }\n",
    "\n",
    "def triage_ticket(ticket_text: str, ticket_id: str = \"ad_hoc\", top_k_tags: int = 5, include_summary: bool = True) -> Dict:\n",
    "    clean_text = normalize_ticket_text(ticket_text)\n",
    "    if not clean_text:\n",
    "        raise ValueError(\"ticket_text is empty after preprocessing\")\n",
    "\n",
    "    department_pred = _predict_label(\"department\", clean_text)\n",
    "    urgency_pred = _predict_label(\"urgency\", clean_text)\n",
    "    tags = extract_tags(clean_text, top_k=top_k_tags)\n",
    "    summary = summarize_text(clean_text) if include_summary else \"\"\n",
    "\n",
    "    return {\n",
    "        \"ticket_id\": str(ticket_id),\n",
    "        \"department\": department_pred,\n",
    "        \"urgency\": urgency_pred,\n",
    "        \"tags\": tags,\n",
    "        \"summary\": summary,\n",
    "    }\n",
    "\n",
    "print(\"Unified triage function ready: triage_ticket(ticket_text)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a795f204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 25: Run demo on 5-10 sample tickets + save sample predictions\n",
    "import json\n",
    "\n",
    "DEMO_SIZE = 8\n",
    "DEMO_INCLUDE_SUMMARY = True\n",
    "id_col = CONFIG[\"id_column\"]\n",
    "\n",
    "working = df_raw[[id_col, \"ticket_text\"]].copy()\n",
    "working[\"word_count\"] = working[\"ticket_text\"].str.split().str.len()\n",
    "\n",
    "long_idx = working.nlargest(2, \"word_count\").index.tolist()\n",
    "question_mask = working[\"ticket_text\"].str.contains(r\"\\?\", regex=True, na=False)\n",
    "question_pool = working[question_mask]\n",
    "question_idx = question_pool.sample(n=min(2, len(question_pool)), random_state=CONFIG[\"seed\"]).index.tolist() if len(question_pool) > 0 else []\n",
    "\n",
    "remaining = max(0, DEMO_SIZE - len(long_idx) - len(question_idx))\n",
    "random_pool = working.drop(index=set(long_idx + question_idx), errors=\"ignore\")\n",
    "rand_idx = random_pool.sample(n=min(remaining, len(random_pool)), random_state=CONFIG[\"seed\"]).index.tolist()\n",
    "\n",
    "demo_idx = list(dict.fromkeys(long_idx + question_idx + rand_idx))\n",
    "demo_df = working.loc[demo_idx].reset_index(drop=True)\n",
    "\n",
    "demo_outputs = []\n",
    "demo_summary_mode = \"model\"\n",
    "for row in demo_df.itertuples(index=False):\n",
    "    try:\n",
    "        result = triage_ticket(\n",
    "            ticket_text=row.ticket_text,\n",
    "            ticket_id=row[0],\n",
    "            top_k_tags=5,\n",
    "            include_summary=DEMO_INCLUDE_SUMMARY,\n",
    "        )\n",
    "    except Exception:\n",
    "        demo_summary_mode = \"fallback_no_summary\"\n",
    "        result = triage_ticket(\n",
    "            ticket_text=row.ticket_text,\n",
    "            ticket_id=row[0],\n",
    "            top_k_tags=5,\n",
    "            include_summary=False,\n",
    "        )\n",
    "    demo_outputs.append(result)\n",
    "\n",
    "print(f\"Demo tickets processed: {len(demo_outputs)}\")\n",
    "print(f\"Demo summary mode: {demo_summary_mode}\")\n",
    "for i, payload in enumerate(demo_outputs[:3], start=1):\n",
    "    print(f\"\\n--- Demo Output {i} ---\")\n",
    "    print(json.dumps(payload, ensure_ascii=True, indent=2))\n",
    "\n",
    "sample_jsonl_path = RESULTS_DIR / \"sample_predictions.jsonl\"\n",
    "with open(sample_jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for payload in demo_outputs:\n",
    "        f.write(json.dumps(payload, ensure_ascii=True) + \"\\n\")\n",
    "print(f\"Saved demo predictions: {sample_jsonl_path}\")\n",
    "\n",
    "demo_view = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"ticket_id\": p[\"ticket_id\"],\n",
    "            \"department\": p[\"department\"][\"label\"],\n",
    "            \"dept_conf\": p[\"department\"][\"confidence\"],\n",
    "            \"urgency\": p[\"urgency\"][\"label\"],\n",
    "            \"urg_conf\": p[\"urgency\"][\"confidence\"],\n",
    "            \"tags\": \", \".join(p[\"tags\"]),\n",
    "            \"summary\": p[\"summary\"],\n",
    "        }\n",
    "        for p in demo_outputs\n",
    "    ]\n",
    ")\n",
    "display(demo_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9a72b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 26: Business value analysis + summary-length comparison\n",
    "import time\n",
    "\n",
    "ASSUMPTIONS = {\n",
    "    \"reading_speed_wpm\": 200,\n",
    "    \"routing_buffer_seconds\": 30,\n",
    "    \"model_seconds_per_ticket\": 5,\n",
    "    \"hourly_wage_usd\": 28.0,\n",
    "    \"monthly_ticket_volume\": int(len(df_raw)),\n",
    "    \"summary_eval_sample_size\": min(100, int(len(df_raw))),\n",
    "}\n",
    "\n",
    "id_col = CONFIG[\"id_column\"]\n",
    "\n",
    "word_counts = df_raw[\"ticket_text\"].str.split().str.len()\n",
    "avg_ticket_words = float(word_counts.mean())\n",
    "\n",
    "manual_read_seconds = (avg_ticket_words / ASSUMPTIONS[\"reading_speed_wpm\"]) * 60.0\n",
    "manual_total_seconds = manual_read_seconds + ASSUMPTIONS[\"routing_buffer_seconds\"]\n",
    "model_total_seconds = float(ASSUMPTIONS[\"model_seconds_per_ticket\"])\n",
    "time_saved_seconds = manual_total_seconds - model_total_seconds\n",
    "\n",
    "hourly_wage = ASSUMPTIONS[\"hourly_wage_usd\"]\n",
    "monthly_volume = ASSUMPTIONS[\"monthly_ticket_volume\"]\n",
    "\n",
    "monthly_hours_saved = (time_saved_seconds * monthly_volume) / 3600.0\n",
    "monthly_savings_usd = monthly_hours_saved * hourly_wage\n",
    "annual_savings_usd = monthly_savings_usd * 12.0\n",
    "\n",
    "# Summary-length analysis on a fixed sample for runtime practicality.\n",
    "sample_n = ASSUMPTIONS[\"summary_eval_sample_size\"]\n",
    "summary_sample = df_raw[[id_col, \"ticket_text\"]].sample(n=sample_n, random_state=CONFIG[\"seed\"]).reset_index(drop=True)\n",
    "\n",
    "summary_outputs = []\n",
    "start_ts = time.time()\n",
    "summary_mode = \"model\"\n",
    "for txt in summary_sample[\"ticket_text\"]:\n",
    "    try:\n",
    "        summary_outputs.append(summarize_text(txt))\n",
    "    except Exception:\n",
    "        summary_mode = \"fallback\"\n",
    "        fallback = \" \".join(normalize_ticket_text(txt).split()[:40])\n",
    "        summary_outputs.append(fallback)\n",
    "elapsed_summary_sec = time.time() - start_ts\n",
    "\n",
    "summary_sample[\"summary_text\"] = summary_outputs\n",
    "summary_sample[\"input_words\"] = summary_sample[\"ticket_text\"].str.split().str.len()\n",
    "summary_sample[\"summary_words\"] = summary_sample[\"summary_text\"].str.split().str.len()\n",
    "\n",
    "avg_input_words_sample = float(summary_sample[\"input_words\"].mean())\n",
    "avg_summary_words_sample = float(summary_sample[\"summary_words\"].mean())\n",
    "compression_ratio = avg_summary_words_sample / max(avg_input_words_sample, 1e-9)\n",
    "\n",
    "business_value_report = {\n",
    "    \"assumptions\": ASSUMPTIONS,\n",
    "    \"dataset_stats\": {\n",
    "        \"num_tickets\": int(len(df_raw)),\n",
    "        \"avg_ticket_words\": round(avg_ticket_words, 4),\n",
    "    },\n",
    "    \"timing_seconds\": {\n",
    "        \"manual_read_seconds\": round(manual_read_seconds, 4),\n",
    "        \"manual_total_seconds\": round(manual_total_seconds, 4),\n",
    "        \"model_total_seconds\": round(model_total_seconds, 4),\n",
    "        \"time_saved_seconds_per_ticket\": round(time_saved_seconds, 4),\n",
    "    },\n",
    "    \"savings_usd\": {\n",
    "        \"monthly_hours_saved\": round(monthly_hours_saved, 4),\n",
    "        \"monthly_savings\": round(monthly_savings_usd, 2),\n",
    "        \"annual_savings\": round(annual_savings_usd, 2),\n",
    "    },\n",
    "    \"summary_length_analysis\": {\n",
    "        \"summary_mode\": summary_mode,\n",
    "        \"sample_size\": int(sample_n),\n",
    "        \"avg_input_words_sample\": round(avg_input_words_sample, 4),\n",
    "        \"avg_summary_words_sample\": round(avg_summary_words_sample, 4),\n",
    "        \"compression_ratio_summary_over_input\": round(compression_ratio, 4),\n",
    "        \"summary_eval_elapsed_seconds\": round(elapsed_summary_sec, 4),\n",
    "    },\n",
    "}\n",
    "\n",
    "business_path = RESULTS_DIR / \"business_value_analysis.json\"\n",
    "with open(business_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(business_value_report, f, ensure_ascii=True, indent=2)\n",
    "\n",
    "print(\"Business Value Summary\")\n",
    "print(f\"- Avg ticket words (full dataset): {avg_ticket_words:.2f}\")\n",
    "print(f\"- Manual triage time/ticket: {manual_total_seconds:.2f}s\")\n",
    "print(f\"- Model triage time/ticket: {model_total_seconds:.2f}s\")\n",
    "print(f\"- Time saved per ticket: {time_saved_seconds:.2f}s\")\n",
    "print(f\"- Monthly hours saved: {monthly_hours_saved:.2f}h\")\n",
    "print(f\"- Monthly savings (USD): ${monthly_savings_usd:,.2f}\")\n",
    "print(f\"- Annual savings (USD): ${annual_savings_usd:,.2f}\")\n",
    "print(f\"- Avg input words (summary sample): {avg_input_words_sample:.2f}\")\n",
    "print(f\"- Avg summary words (summary sample): {avg_summary_words_sample:.2f}\")\n",
    "print(f\"- Summary/Input ratio: {compression_ratio:.3f}\")\n",
    "print(f\"Saved business report: {business_path}\")\n",
    "\n",
    "display(summary_sample[[id_col, \"input_words\", \"summary_words\"]].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a2c9f9-0626-46e9-870a-3e32d318a236",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61154be4-3c27-4fa3-bf95-cc9ae978a3d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
